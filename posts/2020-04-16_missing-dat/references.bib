
@article{frangakis_addressing_1999,
	title = {Addressing complications of intention-to-treat analysis in the combined presence of all-or-none treatment-noncompliance and subsequent missing outcomes},
	volume = {86},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/86.2.365},
	doi = {10.1093/biomet/86.2.365},
	abstract = {We study the combined impact that all-or-none compliance and subsequent missing outcomes can have on the estimation of the intention-to-treat effect of assignment in randomised studies. In this setting, a standard analysis, which drops subjects with missing outcomes and ignores compliance information, can be biased for the intention-to-treat effect. To address all-or-none compliance that is followed by missing outcomes, we construct a new estimation procedure for the intention-to-treat effect that maintains good randomisation-based properties under more plausible, nonignorable noncompliance and nonignorable missing-outcome conditions: the 'compound exclusion restriction' on the effect of assignment and the 'latent ignorability' of the missing data mechanism. We present both theoretical results and a simulation study. Moreover, we show how the two key concepts of compound exclusion and latent ignorability are relevant in more complicated settings, such as right censoring of a time-to-event outcome.},
	language = {en},
	number = {2},
	urldate = {2019-02-12},
	journal = {Biometrika},
	author = {Frangakis, C. and Rubin, D. B.},
	month = jun,
	year = {1999},
	pages = {365--379},
	file = {Frangakis - 1999 - Addressing complications of intention-to-treat ana.pdf:/Users/meghajoshi/Zotero/storage/U2JU9KJ2/Frangakis - 1999 - Addressing complications of intention-to-treat ana.pdf:application/pdf;Friedman (2002).pdf:/Users/meghajoshi/Zotero/storage/WR3VDWJM/Friedman (2002).pdf:application/pdf}
}

@article{kapelner_prediction_2013,
	title = {Prediction with {Missing} {Data} via {Bayesian} {Additive} {Regression} {Trees}},
	url = {http://arxiv.org/abs/1306.0618},
	abstract = {We present a method for incorporating missing data into general forecasting problems which use non-parametric statistical learning. We focus on a tree-based method, Bayesian Additive Regression Trees (BART), enhanced with “Missingness Incorporated in Attributes,” an approach recently proposed for incorporating missingness into decision trees. This procedure extends the native partitioning mechanisms found in tree-based models and does not require imputation. Simulations on generated models and real data indicate that our procedure oﬀers promise for both selection model and pattern mixture frameworks as measured by out-of-sample predictive accuracy. We also illustrate BART’s abilities to incorporate missingness into uncertainty intervals. Our implementation is readily available in the R package bartMachine.},
	language = {en},
	urldate = {2019-02-12},
	journal = {arXiv:1306.0618 [cs, stat]},
	author = {Kapelner, Adam and Bleich, Justin},
	month = jun,
	year = {2013},
	note = {arXiv: 1306.0618},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 18 pages, 3 figures, 2 tables, 1 algorithm},
	file = {Kapelner and Bleich - 2013 - Prediction with Missing Data via Bayesian Additive.pdf:/Users/meghajoshi/Zotero/storage/9I3PQKVD/Kapelner and Bleich - 2013 - Prediction with Missing Data via Bayesian Additive.pdf:application/pdf}
}

@article{leyrat_propensity_2019,
	title = {Propensity score analysis with partially observed covariates: {How} should multiple imputation be used?},
	volume = {28},
	issn = {0962-2802, 1477-0334},
	shorttitle = {Propensity score analysis with partially observed covariates},
	url = {http://journals.sagepub.com/doi/10.1177/0962280217713032},
	doi = {10.1177/0962280217713032},
	abstract = {Inverse probability of treatment weighting is a popular propensity score-based approach to estimate marginal treatment effects in observational studies at risk of confounding bias. A major issue when estimating the propensity score is the presence of partially observed covariates. Multiple imputation is a natural approach to handle missing data on covariates: covariates are imputed and a propensity score analysis is performed in each imputed dataset to estimate the treatment effect. The treatment effect estimates from each imputed dataset are then combined to obtain an overall estimate. We call this method MIte. However, an alternative approach has been proposed, in which the propensity scores are combined across the imputed datasets (MIps). Therefore, there are remaining uncertainties about how to implement multiple imputation for propensity score analysis: (a) should we apply Rubin’s rules to the inverse probability of treatment weighting treatment effect estimates or to the propensity score estimates themselves? (b) does the outcome have to be included in the imputation model? (c) how should we estimate the variance of the inverse probability of treatment weighting estimator after multiple imputation? We studied the consistency and balancing properties of the MIte and MIps estimators and performed a simulation study to empirically assess their performance for the analysis of a binary outcome. We also compared the performance of these methods to complete case analysis and the missingness pattern approach, which uses a different propensity score model for each pattern of missingness, and a third multiple imputation approach in which the propensity score parameters are combined rather than the propensity scores themselves (MIpar). Under a missing at random mechanism, complete case and missingness pattern analyses were biased in most cases for estimating the marginal treatment effect, whereas multiple imputation approaches were approximately unbiased as long as the outcome was included in the imputation model. Only MIte was unbiased in all the studied scenarios and Rubin’s rules provided good variance estimates for MIte. The propensity score estimated in the MIte approach showed good balancing properties. In conclusion, when using multiple imputation in the inverse probability of treatment weighting context, MIte with the outcome included in the imputation model is the preferred approach.},
	language = {en},
	number = {1},
	urldate = {2019-02-12},
	journal = {Statistical Methods in Medical Research},
	author = {Leyrat, Clémence and Seaman, Shaun R and White, Ian R and Douglas, Ian and Smeeth, Liam and Kim, Joseph and Resche-Rigon, Matthieu and Carpenter, James R and Williamson, Elizabeth J},
	month = jan,
	year = {2019},
	pages = {3--19},
	file = {Leyrat et al. - 2019 - Propensity score analysis with partially observed .pdf:/Users/meghajoshi/Zotero/storage/FY9RMF4X/Leyrat et al. - 2019 - Propensity score analysis with partially observed .pdf:application/pdf}
}

@article{lu_propensity_2018,
	title = {Propensity {Score} {Matching} {Analysis} for {Causal} {Effects} with {MNAR} {Covariates}},
	issn = {10170405},
	url = {http://www3.stat.sinica.edu.tw/statistica/J28N4/J28N416/J28N416.html},
	doi = {10.5705/ss.202016.0320},
	abstract = {In observational studies, propensity score methods are popular for estimating causal effects. With completely observed data, this approach is valid under several assumptions; however, in practice data are often missing which can have a substantial impact on the estimation. Current remedies to deal with missing covariates in propensity score methods generally fall into two categories. Some authors propose to account for the missing data patterns in propensity score estimation. Others propose to ﬁrst impute the missing data, then utilize conventional propensity score adjustment methods. Both approaches assume that the data are missing at random (MAR), and there is little discussion regarding the impact on treatment effect estimation if covariates are missing not at random (MNAR). In this paper, we ﬁrst examine the implication of the MAR assumption under the potential outcome framework. We then propose a sensitivity analysis method for assessing the impact of a MNAR covariate on treatment effect estimation with a matching estimator, with varying magnitudes of unmeasured confounding effect due to the missing covariate. Our method takes full advantage of the information contained in the partially missing covariate by matching on the observed portion and identifying a bounding distribution for the missing portion. It can be interpreted similarly as Rosenbaum’s sensitivity analysis, and the results are robust since we make few parametric assumptions. We illustrate the application of the method using the 2012 Ohio Medicaid Assessment Survey (OMAS) to investigate the effect of health insurance on health outcomes, where an important covariate, household income, is partially missing.},
	language = {en},
	urldate = {2019-02-12},
	journal = {Statistica Sinica},
	author = {Lu, Bo and Ashmead, Robert},
	year = {2018},
	file = {Lu and Ashmead - 2018 - Propensity Score Matching Analysis for Causal Effe.pdf:/Users/meghajoshi/Zotero/storage/NJE7CI6M/Lu and Ashmead - 2018 - Propensity Score Matching Analysis for Causal Effe.pdf:application/pdf}
}

@article{mitra_comparison_2016,
	title = {A comparison of two methods of estimating propensity scores after multiple imputation},
	volume = {25},
	issn = {0962-2802, 1477-0334},
	url = {http://journals.sagepub.com/doi/10.1177/0962280212445945},
	doi = {10.1177/0962280212445945},
	abstract = {In many observational studies, analysts estimate treatment effects using propensity scores, e.g. by matching or sub-classifying on the scores. When some values of the covariates are missing, analysts can use multiple imputation to fill in the missing data, estimate propensity scores based on the m completed datasets, and use the propensity scores to estimate treatment effects. We compare two approaches to implement this process. In the first, the analyst estimates the treatment effect using propensity score matching within each completed data set, and averages the m treatment effect estimates. In the second approach, the analyst averages the m propensity scores for each record across the completed datasets, and performs propensity score matching with these averaged scores to estimate the treatment effect. We compare properties of both methods via simulation studies using artificial and real data. The simulations suggest that the second method has greater potential to produce substantial bias reductions than the first, particularly when the missing values are predictive of treatment assignment.},
	language = {en},
	number = {1},
	urldate = {2019-02-12},
	journal = {Statistical Methods in Medical Research},
	author = {Mitra, Robin and Reiter, Jerome P},
	month = feb,
	year = {2016},
	pages = {188--204},
	file = {Mitra and Reiter - 2016 - A comparison of two methods of estimating propensi.pdf:/Users/meghajoshi/Zotero/storage/Z8PEFTM3/Mitra and Reiter - 2016 - A comparison of two methods of estimating propensi.pdf:application/pdf}
}

@article{mitra_estimating_2011,
	title = {Estimating propensity scores with missing covariate data using general location mixture models},
	volume = {30},
	issn = {02776715},
	url = {http://doi.wiley.com/10.1002/sim.4124},
	doi = {10.1002/sim.4124},
	language = {en},
	number = {6},
	urldate = {2019-02-12},
	journal = {Statistics in Medicine},
	author = {Mitra, Robin and Reiter, Jerome P.},
	month = mar,
	year = {2011},
	pages = {627--641},
	file = {Mitra and Reiter - 2011 - Estimating propensity scores with missing covariat.pdf:/Users/meghajoshi/Zotero/storage/L4WRVYCD/Mitra and Reiter - 2011 - Estimating propensity scores with missing covariat.pdf:application/pdf}
}

@incollection{mohan_graphical_2013,
  author      = {Mohan, Karthika and Pearl, Judea and Tian, Jin},
  title       = "Graphical models for inference with missing data",
  editor      = "C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger",
  booktitle   = "Advances in Neural Information Processing System",
  publisher   = "Curran Associates, Inc.",
  address     = "Red Hook, NY",
  year        = 2013,
  pages       = "1277--1285"
}



@article{qu_propensity_2009,
	title = {Propensity score estimation with missing values using a multiple imputation missingness pattern ({MIMP}) approach},
	volume = {28},
	issn = {02776715, 10970258},
	url = {http://doi.wiley.com/10.1002/sim.3549},
	doi = {10.1002/sim.3549},
	abstract = {Propensity scores have been used widely as a bias reduction method to estimate the treatment effect in nonrandomized studies. Since many covariates are generally included in the model for estimating the propensity scores, the proportion of subjects with at least one missing covariate could be large. While many methods have been proposed for propensity score-based estimation in the presence of missing covariates, little has been published comparing the performance of these methods. In this article we propose a novel method called multiple imputation missingness pattern (MIMP) and compare it with the naive estimator (ignoring propensity score) and three commonly used methods of handling missing covariates in propensity score-based estimation (separate estimation of propensity scores within each pattern of missing data, multiple imputation and discarding missing data) under different mechanisms of missing data and degree of correlation among covariates. Simulation shows that all adjusted estimators are much less biased than the naive estimator. Under certain conditions MIMP provides beneﬁts (smaller bias and mean-squared error) compared with existing alternatives. Copyright 2009 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {9},
	urldate = {2019-02-12},
	journal = {Statistics in Medicine},
	author = {Qu, Yongming and Lipkovich, Ilya},
	month = apr,
	year = {2009},
	pages = {1402--1414},
	file = {Qu and Lipkovich - 2009 - Propensity score estimation with missing values us.pdf:/Users/meghajoshi/Zotero/storage/CQHJPRDJ/Qu and Lipkovich - 2009 - Propensity score estimation with missing values us.pdf:application/pdf}
}

@article{reiter_small-sample_2007,
	title = {Small-sample degrees of freedom for multi-component significance tests with multiple imputation for missing data},
	volume = {94},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/asm028},
	doi = {10.1093/biomet/asm028},
	language = {en},
	number = {2},
	urldate = {2019-02-12},
	journal = {Biometrika},
	author = {Reiter, J. P.},
	month = feb,
	year = {2007},
	pages = {502--508},
	file = {Reiter - 2007 - Small-sample degrees of freedom for multi-componen.pdf:/Users/meghajoshi/Zotero/storage/5XJHTFLA/Reiter - 2007 - Small-sample degrees of freedom for multi-componen.pdf:application/pdf}
}

@article{rosenbaum_central_1983,
	title = {The {Central} {Role} of the {Propensity} {Score} in {Observational} {Studies} for {Causal} {Effects}},
	volume = {70},
	issn = {00063444},
	url = {https://www.jstor.org/stable/2335942?origin=crossref},
	doi = {10.2307/2335942},
	number = {1},
	urldate = {2019-02-12},
	journal = {Biometrika},
	author = {Rosenbaum, Paul R. and Rubin, Donald B.},
	month = apr,
	year = {1983},
	pages = {41},
	file = {Rosenbaum and Rubin - 1983 - The Central Role of the Propensity Score in Observ.pdf:/Users/meghajoshi/Zotero/storage/C6G5ULPY/Rosenbaum and Rubin - 1983 - The Central Role of the Propensity Score in Observ.pdf:application/pdf}
}

@article{noauthor_reducing_nodate,
	title = {Reducing {Bias} in {Observational} {Studies} {Using} {Subclassification} on the {Propensity} {Score}},
	language = {en},
	pages = {10},
	file = {Reducing Bias in Observational Studies Using Subcl.pdf:/Users/meghajoshi/Zotero/storage/DQ7HUINK/Reducing Bias in Observational Studies Using Subcl.pdf:application/pdf}
}

@article{song_handling_nodate,
	title = {Handling {Baseline} {Differences} and {Missing} {Items} in a {Longitudinal} {Study} of {HIV} {Risk} {Among} {Runaway} {Youths}},
	abstract = {Many longitudinal studies in ﬁeld settings present challenges due to selection bias and incomplete data. A motivating example is provided by an intervention study aimed at preventing HIV transmission among runaway youths housed at shelters in New York City. Two shelters with 167 youths received the intervention, and two shelters with 144 youths received the control treatment. The number of unprotected sexual acts in the prior three months for each youth was assessed at a baseline interview and (to the extent possible) at ﬁve follow-up time points. Among observed items, there is strong evidence of a lack of balance on baseline characteristics between the intervention and control groups; meanwhile, beyond occasional missing items among participants interviewed at baseline, there were three items about baseline characteristics added after the study began, resulting in a few items being missing on a large percentage of the study sample. Here, we outline two strategies for handling the complexities of this data set, both of which make use of propensity scores to address the imbalances across treatment groups. One approach relies on available cases and ad hoc choices to simplify the steps leading up to a linear mixed model analysis in SAS PROC MIXED; the other approach uses multiple imputation strategies to reﬂect uncertainty due to missing values in an analogous linear mixed model analysis. Ultimately we did not ﬁnd substantial qualitative differences in this setting between the available-case and imputed-data approaches. But in both cases, we ﬁnd that the considerable imbalance on covariates between treatment arms constrains the ability to draw inferences about the intervention effect, suggesting the importance of evaluating propensity-score distributions in quasi-experimental intervention research.},
	language = {en},
	author = {Song, Juwon and Belin, Thomas R and Lee, Martha B and Gao, Xingyu and Rotheram-Borus, Mary Jane},
	pages = {13},
	file = {Song et al. - Handling Baseline Differences and Missing Items in.pdf:/Users/meghajoshi/Zotero/storage/KKNM3E6D/Song et al. - Handling Baseline Differences and Missing Items in.pdf:application/pdf}
}

@article{stuart_matching_2010,
	title = {Matching {Methods} for {Causal} {Inference}: {A} {Review} and a {Look} {Forward}},
	volume = {25},
	issn = {0883-4237},
	shorttitle = {Matching {Methods} for {Causal} {Inference}},
	url = {http://projecteuclid.org/euclid.ss/1280841730},
	doi = {10.1214/09-STS313},
	abstract = {When estimating causal effects using observational data, it is desirable to replicate a randomized experiment as closely as possible by obtaining treated and control groups with similar covariate distributions. This goal can often be achieved by choosing well-matched samples of the original treated and control groups, thereby reducing bias due to the covariates. Since the 1970’s, work on matching methods has examined how to best choose treated and control subjects for comparison. Matching methods are gaining popularity in fields such as economics, epidemiology, medicine, and political science. However, until now the literature and related advice has been scattered across disciplines. Researchers who are interested in using matching methods–or developing methods related to matching–do not have a single place to turn to learn about past and current research. This paper provides a structure for thinking about matching methods and guidance on their use, coalescing the existing research (both old and new) and providing a summary of where the literature on matching methods is now and where it should be headed.},
	language = {en},
	number = {1},
	urldate = {2019-02-12},
	journal = {Statistical Science},
	author = {Stuart, Elizabeth A.},
	month = feb,
	year = {2010},
	pages = {1--21},
	file = {Stuart - 2010 - Matching Methods for Causal Inference A Review an.pdf:/Users/meghajoshi/Zotero/storage/YBM8W9DY/Stuart - 2010 - Matching Methods for Causal Inference A Review an.pdf:application/pdf}
}

@article{frangakis_addressing_1999-1,
	title = {Addressing complications of intention-to-treat analysis in the combined presence of all-or-none treatment-noncompliance and subsequent missing outcomes},
	volume = {86},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/86.2.365},
	doi = {10.1093/biomet/86.2.365},
	abstract = {We study the combined impact that all-or-none compliance and subsequent missing outcomes can have on the estimation of the intention-to-treat effect of assignment in randomised studies. In this setting, a standard analysis, which drops subjects with missing outcomes and ignores compliance information, can be biased for the intention-to-treat effect. To address all-or-none compliance that is followed by missing outcomes, we construct a new estimation procedure for the intention-to-treat effect that maintains good randomisation-based properties under more plausible, nonignorable noncompliance and nonignorable missing-outcome conditions: the 'compound exclusion restriction' on the effect of assignment and the 'latent ignorability' of the missing data mechanism. We present both theoretical results and a simulation study. Moreover, we show how the two key concepts of compound exclusion and latent ignorability are relevant in more complicated settings, such as right censoring of a time-to-event outcome.},
	language = {en},
	number = {2},
	urldate = {2019-02-12},
	journal = {Biometrika},
	author = {Frangakis, C.},
	month = jun,
	year = {1999},
	pages = {365--379},
	file = {Frangakis - 1999 - Addressing complications of intention-to-treat ana.pdf:/Users/meghajoshi/Zotero/storage/MT3GC9BR/Frangakis - 1999 - Addressing complications of intention-to-treat ana.pdf:application/pdf}
}

@article{kapelner_prediction_2013-1,
	title = {Prediction with {Missing} {Data} via {Bayesian} {Additive} {Regression} {Trees}},
	url = {http://arxiv.org/abs/1306.0618},
	abstract = {We present a method for incorporating missing data into general forecasting problems which use non-parametric statistical learning. We focus on a tree-based method, Bayesian Additive Regression Trees (BART), enhanced with “Missingness Incorporated in Attributes,” an approach recently proposed for incorporating missingness into decision trees. This procedure extends the native partitioning mechanisms found in tree-based models and does not require imputation. Simulations on generated models and real data indicate that our procedure oﬀers promise for both selection model and pattern mixture frameworks as measured by out-of-sample predictive accuracy. We also illustrate BART’s abilities to incorporate missingness into uncertainty intervals. Our implementation is readily available in the R package bartMachine.},
	language = {en},
	urldate = {2019-02-12},
	journal = {arXiv:1306.0618 [cs, stat]},
	author = {Kapelner, Adam and Bleich, Justin},
	month = jun,
	year = {2013},
	note = {arXiv: 1306.0618},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 18 pages, 3 figures, 2 tables, 1 algorithm},
	file = {Kapelner and Bleich - 2013 - Prediction with Missing Data via Bayesian Additive.pdf:/Users/meghajoshi/Zotero/storage/FL79LIL8/Kapelner and Bleich - 2013 - Prediction with Missing Data via Bayesian Additive.pdf:application/pdf}
}

@article{leyrat_propensity_2019-1,
	title = {Propensity score analysis with partially observed covariates: {How} should multiple imputation be used?},
	volume = {28},
	issn = {0962-2802, 1477-0334},
	shorttitle = {Propensity score analysis with partially observed covariates},
	url = {http://journals.sagepub.com/doi/10.1177/0962280217713032},
	doi = {10.1177/0962280217713032},
	abstract = {Inverse probability of treatment weighting is a popular propensity score-based approach to estimate marginal treatment effects in observational studies at risk of confounding bias. A major issue when estimating the propensity score is the presence of partially observed covariates. Multiple imputation is a natural approach to handle missing data on covariates: covariates are imputed and a propensity score analysis is performed in each imputed dataset to estimate the treatment effect. The treatment effect estimates from each imputed dataset are then combined to obtain an overall estimate. We call this method MIte. However, an alternative approach has been proposed, in which the propensity scores are combined across the imputed datasets (MIps). Therefore, there are remaining uncertainties about how to implement multiple imputation for propensity score analysis: (a) should we apply Rubin’s rules to the inverse probability of treatment weighting treatment effect estimates or to the propensity score estimates themselves? (b) does the outcome have to be included in the imputation model? (c) how should we estimate the variance of the inverse probability of treatment weighting estimator after multiple imputation? We studied the consistency and balancing properties of the MIte and MIps estimators and performed a simulation study to empirically assess their performance for the analysis of a binary outcome. We also compared the performance of these methods to complete case analysis and the missingness pattern approach, which uses a different propensity score model for each pattern of missingness, and a third multiple imputation approach in which the propensity score parameters are combined rather than the propensity scores themselves (MIpar). Under a missing at random mechanism, complete case and missingness pattern analyses were biased in most cases for estimating the marginal treatment effect, whereas multiple imputation approaches were approximately unbiased as long as the outcome was included in the imputation model. Only MIte was unbiased in all the studied scenarios and Rubin’s rules provided good variance estimates for MIte. The propensity score estimated in the MIte approach showed good balancing properties. In conclusion, when using multiple imputation in the inverse probability of treatment weighting context, MIte with the outcome included in the imputation model is the preferred approach.},
	language = {en},
	number = {1},
	urldate = {2019-02-12},
	journal = {Statistical Methods in Medical Research},
	author = {Leyrat, Clémence and Seaman, Shaun R and White, Ian R and Douglas, Ian and Smeeth, Liam and Kim, Joseph and Resche-Rigon, Matthieu and Carpenter, James R and Williamson, Elizabeth J},
	month = jan,
	year = {2019},
	pages = {3--19},
	file = {Leyrat et al. - 2019 - Propensity score analysis with partially observed .pdf:/Users/meghajoshi/Zotero/storage/HTGMNKD9/Leyrat et al. - 2019 - Propensity score analysis with partially observed .pdf:application/pdf}
}

@article{lu_propensity_2018-1,
	title = {Propensity {Score} {Matching} {Analysis} for {Causal} {Effects} with {MNAR} {Covariates}},
	issn = {10170405},
	url = {http://www3.stat.sinica.edu.tw/statistica/J28N4/J28N416/J28N416.html},
	doi = {10.5705/ss.202016.0320},
	abstract = {In observational studies, propensity score methods are popular for estimating causal effects. With completely observed data, this approach is valid under several assumptions; however, in practice data are often missing which can have a substantial impact on the estimation. Current remedies to deal with missing covariates in propensity score methods generally fall into two categories. Some authors propose to account for the missing data patterns in propensity score estimation. Others propose to ﬁrst impute the missing data, then utilize conventional propensity score adjustment methods. Both approaches assume that the data are missing at random (MAR), and there is little discussion regarding the impact on treatment effect estimation if covariates are missing not at random (MNAR). In this paper, we ﬁrst examine the implication of the MAR assumption under the potential outcome framework. We then propose a sensitivity analysis method for assessing the impact of a MNAR covariate on treatment effect estimation with a matching estimator, with varying magnitudes of unmeasured confounding effect due to the missing covariate. Our method takes full advantage of the information contained in the partially missing covariate by matching on the observed portion and identifying a bounding distribution for the missing portion. It can be interpreted similarly as Rosenbaum’s sensitivity analysis, and the results are robust since we make few parametric assumptions. We illustrate the application of the method using the 2012 Ohio Medicaid Assessment Survey (OMAS) to investigate the effect of health insurance on health outcomes, where an important covariate, household income, is partially missing.},
	language = {en},
	urldate = {2019-02-12},
	journal = {Statistica Sinica},
	author = {Lu, Bo and Ashmead, Robert},
	year = {2018},
	file = {Lu and Ashmead - 2018 - Propensity Score Matching Analysis for Causal Effe.pdf:/Users/meghajoshi/Zotero/storage/4UD6RPM5/Lu and Ashmead - 2018 - Propensity Score Matching Analysis for Causal Effe.pdf:application/pdf}
}

@article{mitra_comparison_2016-1,
	title = {A comparison of two methods of estimating propensity scores after multiple imputation},
	volume = {25},
	issn = {0962-2802, 1477-0334},
	url = {http://journals.sagepub.com/doi/10.1177/0962280212445945},
	doi = {10.1177/0962280212445945},
	abstract = {In many observational studies, analysts estimate treatment effects using propensity scores, e.g. by matching or sub-classifying on the scores. When some values of the covariates are missing, analysts can use multiple imputation to fill in the missing data, estimate propensity scores based on the m completed datasets, and use the propensity scores to estimate treatment effects. We compare two approaches to implement this process. In the first, the analyst estimates the treatment effect using propensity score matching within each completed data set, and averages the m treatment effect estimates. In the second approach, the analyst averages the m propensity scores for each record across the completed datasets, and performs propensity score matching with these averaged scores to estimate the treatment effect. We compare properties of both methods via simulation studies using artificial and real data. The simulations suggest that the second method has greater potential to produce substantial bias reductions than the first, particularly when the missing values are predictive of treatment assignment.},
	language = {en},
	number = {1},
	urldate = {2019-02-12},
	journal = {Statistical Methods in Medical Research},
	author = {Mitra, Robin and Reiter, Jerome P},
	month = feb,
	year = {2016},
	pages = {188--204},
	file = {Mitra and Reiter - 2016 - A comparison of two methods of estimating propensi.pdf:/Users/meghajoshi/Zotero/storage/9R4KJGD8/Mitra and Reiter - 2016 - A comparison of two methods of estimating propensi.pdf:application/pdf}
}

@article{mitra_estimating_2011-1,
	title = {Estimating propensity scores with missing covariate data using general location mixture models},
	volume = {30},
	issn = {02776715},
	url = {http://doi.wiley.com/10.1002/sim.4124},
	doi = {10.1002/sim.4124},
	language = {en},
	number = {6},
	urldate = {2019-02-12},
	journal = {Statistics in Medicine},
	author = {Mitra, Robin and Reiter, Jerome P.},
	month = mar,
	year = {2011},
	pages = {627--641},
	file = {Mitra and Reiter - 2011 - Estimating propensity scores with missing covariat.pdf:/Users/meghajoshi/Zotero/storage/R6MFX99Z/Mitra and Reiter - 2011 - Estimating propensity scores with missing covariat.pdf:application/pdf}
}



@article{qu_propensity_2009-1,
	title = {Propensity score estimation with missing values using a multiple imputation missingness pattern ({MIMP}) approach},
	volume = {28},
	issn = {02776715, 10970258},
	url = {http://doi.wiley.com/10.1002/sim.3549},
	doi = {10.1002/sim.3549},
	abstract = {Propensity scores have been used widely as a bias reduction method to estimate the treatment effect in nonrandomized studies. Since many covariates are generally included in the model for estimating the propensity scores, the proportion of subjects with at least one missing covariate could be large. While many methods have been proposed for propensity score-based estimation in the presence of missing covariates, little has been published comparing the performance of these methods. In this article we propose a novel method called multiple imputation missingness pattern (MIMP) and compare it with the naive estimator (ignoring propensity score) and three commonly used methods of handling missing covariates in propensity score-based estimation (separate estimation of propensity scores within each pattern of missing data, multiple imputation and discarding missing data) under different mechanisms of missing data and degree of correlation among covariates. Simulation shows that all adjusted estimators are much less biased than the naive estimator. Under certain conditions MIMP provides beneﬁts (smaller bias and mean-squared error) compared with existing alternatives. Copyright 2009 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {9},
	urldate = {2019-02-12},
	journal = {Statistics in Medicine},
	author = {Qu, Yongming and Lipkovich, Ilya},
	month = apr,
	year = {2009},
	pages = {1402--1414},
	file = {Qu and Lipkovich - 2009 - Propensity score estimation with missing values us.pdf:/Users/meghajoshi/Zotero/storage/5XJS4V2K/Qu and Lipkovich - 2009 - Propensity score estimation with missing values us.pdf:application/pdf}
}

@article{reiter_small-sample_2007-1,
	title = {Small-sample degrees of freedom for multi-component significance tests with multiple imputation for missing data},
	volume = {94},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/asm028},
	doi = {10.1093/biomet/asm028},
	language = {en},
	number = {2},
	urldate = {2019-02-12},
	journal = {Biometrika},
	author = {Reiter, J. P.},
	month = feb,
	year = {2007},
	pages = {502--508},
	file = {Reiter - 2007 - Small-sample degrees of freedom for multi-componen.pdf:/Users/meghajoshi/Zotero/storage/PPTFSZ7Q/Reiter - 2007 - Small-sample degrees of freedom for multi-componen.pdf:application/pdf}
}

@article{rosenbaum_central_1983-1,
	title = {The {Central} {Role} of the {Propensity} {Score} in {Observational} {Studies} for {Causal} {Effects}},
	volume = {70},
	issn = {00063444},
	url = {https://www.jstor.org/stable/2335942?origin=crossref},
	doi = {10.2307/2335942},
	number = {1},
	urldate = {2019-02-12},
	journal = {Biometrika},
	author = {Rosenbaum, Paul R. and Rubin, Donald B.},
	month = apr,
	year = {1983},
	pages = {41},
	file = {Rosenbaum and Rubin - 1983 - The Central Role of the Propensity Score in Observ.pdf:/Users/meghajoshi/Zotero/storage/2NSRHJBC/Rosenbaum and Rubin - 1983 - The Central Role of the Propensity Score in Observ.pdf:application/pdf}
}

@article{noauthor_reducing_nodate-1,
	title = {Reducing {Bias} in {Observational} {Studies} {Using} {Subclassification} on the {Propensity} {Score}},
	language = {en},
	pages = {10},
	file = {Reducing Bias in Observational Studies Using Subcl.pdf:/Users/meghajoshi/Zotero/storage/CLZTMT6P/Reducing Bias in Observational Studies Using Subcl.pdf:application/pdf}
}

@article{song_handling_nodate-1,
	title = {Handling {Baseline} {Differences} and {Missing} {Items} in a {Longitudinal} {Study} of {HIV} {Risk} {Among} {Runaway} {Youths}},
	abstract = {Many longitudinal studies in ﬁeld settings present challenges due to selection bias and incomplete data. A motivating example is provided by an intervention study aimed at preventing HIV transmission among runaway youths housed at shelters in New York City. Two shelters with 167 youths received the intervention, and two shelters with 144 youths received the control treatment. The number of unprotected sexual acts in the prior three months for each youth was assessed at a baseline interview and (to the extent possible) at ﬁve follow-up time points. Among observed items, there is strong evidence of a lack of balance on baseline characteristics between the intervention and control groups; meanwhile, beyond occasional missing items among participants interviewed at baseline, there were three items about baseline characteristics added after the study began, resulting in a few items being missing on a large percentage of the study sample. Here, we outline two strategies for handling the complexities of this data set, both of which make use of propensity scores to address the imbalances across treatment groups. One approach relies on available cases and ad hoc choices to simplify the steps leading up to a linear mixed model analysis in SAS PROC MIXED; the other approach uses multiple imputation strategies to reﬂect uncertainty due to missing values in an analogous linear mixed model analysis. Ultimately we did not ﬁnd substantial qualitative differences in this setting between the available-case and imputed-data approaches. But in both cases, we ﬁnd that the considerable imbalance on covariates between treatment arms constrains the ability to draw inferences about the intervention effect, suggesting the importance of evaluating propensity-score distributions in quasi-experimental intervention research.},
	language = {en},
	author = {Song, Juwon and Belin, Thomas R and Lee, Martha B and Gao, Xingyu and Rotheram-Borus, Mary Jane},
	pages = {13},
	file = {Song et al. - Handling Baseline Differences and Missing Items in.pdf:/Users/meghajoshi/Zotero/storage/8IIN4HQV/Song et al. - Handling Baseline Differences and Missing Items in.pdf:application/pdf}
}

@article{stuart_matching_2010-1,
	title = {Matching {Methods} for {Causal} {Inference}: {A} {Review} and a {Look} {Forward}},
	volume = {25},
	issn = {0883-4237},
	shorttitle = {Matching {Methods} for {Causal} {Inference}},
	url = {http://projecteuclid.org/euclid.ss/1280841730},
	doi = {10.1214/09-STS313},
	abstract = {When estimating causal effects using observational data, it is desirable to replicate a randomized experiment as closely as possible by obtaining treated and control groups with similar covariate distributions. This goal can often be achieved by choosing well-matched samples of the original treated and control groups, thereby reducing bias due to the covariates. Since the 1970’s, work on matching methods has examined how to best choose treated and control subjects for comparison. Matching methods are gaining popularity in fields such as economics, epidemiology, medicine, and political science. However, until now the literature and related advice has been scattered across disciplines. Researchers who are interested in using matching methods–or developing methods related to matching–do not have a single place to turn to learn about past and current research. This paper provides a structure for thinking about matching methods and guidance on their use, coalescing the existing research (both old and new) and providing a summary of where the literature on matching methods is now and where it should be headed.},
	language = {en},
	number = {1},
	urldate = {2019-02-12},
	journal = {Statistical Science},
	author = {Stuart, Elizabeth A.},
	month = feb,
	year = {2010},
	pages = {1--21},
	file = {Stuart - 2010 - Matching Methods for Causal Inference A Review an.pdf:/Users/meghajoshi/Zotero/storage/EXHI3CMC/Stuart - 2010 - Matching Methods for Causal Inference A Review an.pdf:application/pdf}
}

@article{baraldi_introduction_2010,
	title = {An introduction to modern missing data analyses},
	volume = {48},
	issn = {00224405},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022440509000661},
	doi = {10.1016/j.jsp.2009.10.001},
	abstract = {A great deal of recent methodological research has focused on two modern missing data analysis methods: maximum likelihood and multiple imputation. These approaches are advantageous to traditional techniques (e.g. deletion and mean imputation techniques) because they require less stringent assumptions and mitigate the pitfalls of traditional techniques. This article explains the theoretical underpinnings of missing data analyses, gives an overview of traditional missing data techniques, and provides accessible descriptions of maximum likelihood and multiple imputation. In particular, this article focuses on maximum likelihood estimation and presents two analysis examples from the Longitudinal Study of American Youth data. One of these examples includes a description of the use of auxiliary variables. Finally, the paper illustrates ways that researchers can use intentional, or planned, missing data to enhance their research designs.},
	language = {en},
	number = {1},
	urldate = {2019-02-12},
	journal = {Journal of School Psychology},
	author = {Baraldi, Amanda N. and Enders, Craig K.},
	month = feb,
	year = {2010},
	pages = {5--37},
	file = {Baraldi and Enders - 2010 - An introduction to modern missing data analyses.pdf:/Users/meghajoshi/Zotero/storage/EG98DYJU/Baraldi and Enders - 2010 - An introduction to modern missing data analyses.pdf:application/pdf}
}

@article{crowe_comparison_2010,
	title = {Comparison of several imputation methods for missing baseline data in propensity scores analysis of binary outcome},
	volume = {9},
	issn = {15391604},
	url = {http://doi.wiley.com/10.1002/pst.389},
	doi = {10.1002/pst.389},
	language = {en},
	number = {4},
	urldate = {2019-02-12},
	journal = {Pharmaceutical Statistics},
	author = {Crowe, Brenda J. and Lipkovich, Ilya A. and Wang, Ouhong},
	month = oct,
	year = {2010},
	pages = {269--279},
	file = {Crowe et al. - 2010 - Comparison of several imputation methods for missi.pdf:/Users/meghajoshi/Zotero/storage/V2QL2XB2/Crowe et al. - 2010 - Comparison of several imputation methods for missi.pdf:application/pdf;m-graph_JL&DG (1).pdf:/Users/meghajoshi/Zotero/storage/G37SB7RQ/m-graph_JL&DG (1).pdf:application/pdf;Rickles (2011).pdf:/Users/meghajoshi/Zotero/storage/WFKNA8CL/Rickles (2011).pdf:application/pdf;Rosenbaum & Rubin (1983).pdf:/Users/meghajoshi/Zotero/storage/ITCGMIGQ/Rosenbaum & Rubin (1983).pdf:application/pdf;Rubin (1974).pdf:/Users/meghajoshi/Zotero/storage/PN2ZCP87/Rubin (1974).pdf:application/pdf;Rubin (2001).pdf:/Users/meghajoshi/Zotero/storage/DGP4UCZP/Rubin (2001).pdf:application/pdf;Shadish Cook & Campbell (2002).pdf:/Users/meghajoshi/Zotero/storage/ZC2H54WG/Shadish Cook & Campbell (2002).pdf:application/pdf;Stuart (2008).pdf:/Users/meghajoshi/Zotero/storage/C2VB43EL/Stuart (2008).pdf:application/pdf;Stuart (2010).pdf:/Users/meghajoshi/Zotero/storage/GXBYUCPF/Stuart (2010).pdf:application/pdf;Thoemmes & Mohan (2015).pdf:/Users/meghajoshi/Zotero/storage/EK82VAER/Thoemmes & Mohan (2015).pdf:application/pdf}
}

@article{dagostino_estimating_2000,
	title = {Estimating and {Using} {Propensity} {Scores} with {Partially} {Missing} {Data}},
	volume = {95},
	issn = {01621459},
	url = {https://www.jstor.org/stable/2669455?origin=crossref},
	doi = {10.2307/2669455},
	language = {en},
	number = {451},
	urldate = {2019-02-12},
	journal = {Journal of the American Statistical Association},
	author = {D'Agostino, Ralph B. and Rubin, Donald B.},
	month = sep,
	year = {2000},
	pages = {749},
	file = {D'Agostino and Rubin - 2000 - Estimating and Using Propensity Scores with Partia.pdf:/Users/meghajoshi/Zotero/storage/GD4PNH35/D'Agostino and Rubin - 2000 - Estimating and Using Propensity Scores with Partia.pdf:application/pdf}
}

@article{barnard_small-sample_nodate,
	title = {Small-{Sample} {Degrees} of {Freedom} with {Multiple} {Imputation}},
	abstract = {An appealing feature of multiple imputation is the simplicity of the rules for combining the multiple complete-data inferences into a final inference, the repeated-imputation inference (Rubin, 1987). This inference is based on a t distribution and is derived from a Bayesian paradigm under the assumption that the complete-data degrees of freedom, vcom, are infinite, but the number of imputations, m, is finite. When vcom is small and there is only a modest proportion of missing data, the calculated repeated-imputation degrees of freedom, vit, for the t reference distribution can be much larger than vcor, which is clearly inappropriate. Following the Bayesian paradigm, we derive an adjusted degrees of freedom, Vir with the following three properties: for fixed m and estimated fraction of missing information, vn monotonically increases in vcom; Vi is always less than or equal to vcom; and vm equals v. when vcom is infinite. A small simulation study demonstrates the superior frequentist performance when using Vi, rather than v7n.},
	language = {en},
	author = {Barnard, John and Rubin, Donald B},
	pages = {9},
	year = {1999},
	file = {Barnard and Rubin - Small-Sample Degrees of Freedom with Multiple Impu.pdf:/Users/meghajoshi/Zotero/storage/2XBKQCHW/Barnard and Rubin - Small-Sample Degrees of Freedom with Multiple Impu.pdf:application/pdf}
}

@article{murray_multiple_2018,
	title = {Multiple {Imputation}: {A} {Review} of {Practical} and {Theoretical} {Findings}},
	volume = {33},
	issn = {0883-4237},
	shorttitle = {Multiple {Imputation}},
	url = {https://projecteuclid.org/euclid.ss/1525313139},
	doi = {10.1214/18-STS644},
	abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in ﬂexible joint modeling and sequential regression/chained equations/fully conditional speciﬁcation approaches. Finally, we compare and contrast diﬀerent methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
	language = {en},
	number = {2},
	urldate = {2019-02-12},
	journal = {Statistical Science},
	author = {Murray, Jared S.},
	month = may,
	year = {2018},
	pages = {142--159},
	file = {Murray - 2018 - Multiple Imputation A Review of Practical and The.pdf:/Users/meghajoshi/Zotero/storage/F5XUBVBJ/Murray - 2018 - Multiple Imputation A Review of Practical and The.pdf:application/pdf}
}

@article{meng_maximum_nodate,
	title = {Maximum {Likelihood} {Estimation} via the {ECM} {Algorithm}: {A} {General} {Framework}},
	abstract = {Two major reasons for the popularity of the EM algorithm are that its maximum step involves only complete-data maximum likelihood estimation, which is often computationally simple, and that its convergence is stable, with each iteration increasing the likelihood. When the associated complete-data maximum likelihood estimation itself is complicated, EM is less attractive because the M-step is computationally unattractive. In many cases, however, complete-data maximum likelihood estimation is relatively simple when conditional on some function of the parameters being estimated. We introduce a class of generalized EM algorithms, which we call the ECM algorithm, for Expectation/Conditional Maximization (CM), that takes advantage of the simplicity of completedata conditional maximum likelihood estimation by replacing a complicated M-step of EM with several computationally simpler cM-steps. We show that the ECM algorithm shares all the appealing convergence properties of EM, such as always increasing the likelihood, and present several illustrative examples.},
	language = {en},
	author = {Meng, Xiao-Li},
	pages = {13},
	file = {Meng - Maximum Likelihood Estimation via the ECM Algorith.pdf:/Users/meghajoshi/Zotero/storage/NVT38GT8/Meng - Maximum Likelihood Estimation via the ECM Algorith.pdf:application/pdf}
}

@article{mattei_estimating_2009,
	title = {Estimating and using propensity score in presence of missing background data: an application to assess the impact of childbearing on wellbeing},
	volume = {18},
	issn = {1618-2510, 1613-981X},
	shorttitle = {Estimating and using propensity score in presence of missing background data},
	url = {http://link.springer.com/10.1007/s10260-007-0086-0},
	doi = {10.1007/s10260-007-0086-0},
	abstract = {Propensity score methods are an increasingly popular technique for causal inference. To estimate propensity scores, we must model the distribution of the treatment indicator given a vector of covariates. Much work has been done in the case where the covariates are fully observed. Unfortunately, many large scale and complex surveys, such as longitudinal surveys, suffer from missing covariate values. In this paper, we compare three different approaches and their underlying assumptions of handling missing background data in the estimation and use of propensity scores: a completecase analysis, a pattern-mixture model based approach developed by Rosenbaum and Rubin (J Am Stat Assoc79:516–524, 1984), and a multiple imputation approach. We apply these methods to assess the impact of childbearing events on individuals’ wellbeing in Indonesia, using a sample of women from the Indonesia Family Life Survey.},
	language = {en},
	number = {2},
	urldate = {2019-02-12},
	journal = {Statistical Methods and Applications},
	author = {Mattei, Alessandra},
	month = jul,
	year = {2009},
	pages = {257--273},
	file = {Mattei - 2009 - Estimating and using propensity score in presence .pdf:/Users/meghajoshi/Zotero/storage/6J4RKXNP/Mattei - 2009 - Estimating and using propensity score in presence .pdf:application/pdf}
}

@article{alam_should_2018,
	title = {Should a propensity score model be super? {The} utility of ensemble procedures for causal adjustment: {Ensemble} {Procedures} for {Propensity} {Score} {Estimation}},
	issn = {02776715},
	shorttitle = {Should a propensity score model be super?},
	url = {http://doi.wiley.com/10.1002/sim.8075},
	doi = {10.1002/sim.8075},
	language = {en},
	urldate = {2019-02-12},
	journal = {Statistics in Medicine},
	author = {Alam, Shomoita and Moodie, Erica E. M. and Stephens, David A.},
	month = dec,
	year = {2018},
	file = {Alam et al. - 2018 - Should a propensity score model be super The util.pdf:/Users/meghajoshi/Zotero/storage/GE7N7T6Y/Alam et al. - 2018 - Should a propensity score model be super The util.pdf:application/pdf}
}

@incollection{goos_handling_1999,
	address = {Berlin, Heidelberg},
	title = {Handling {Missing} {Data} in {Trees}: {Surrogate} {Splits} or {Statistical} {Imputation}?},
	volume = {1704},
	isbn = {978-3-540-66490-1 978-3-540-48247-5},
	shorttitle = {Handling {Missing} {Data} in {Trees}},
	url = {http://link.springer.com/10.1007/978-3-540-48247-5_38},
	abstract = {In many applications of data mining a - sometimes considerable - part of the data values is missing. This may occur because the data values were simply never entered into the operational systems from which the mining table was constructed, or because for example simple domain checks indicate that entered values are incorrect. Despite the frequent occurrence of missing data, most data mining algorithms handle missing data in a rather ad-hoc way, or simply ignore the problem.},
	language = {en},
	urldate = {2019-02-12},
	booktitle = {Principles of {Data} {Mining} and {Knowledge} {Discovery}},
	publisher = {Springer Berlin Heidelberg},
	author = {Feelders, Ad},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Żytkow, Jan M. and Rauch, Jan},
	year = {1999},
	doi = {10.1007/978-3-540-48247-5_38},
	pages = {329--334},
	file = {Feelders - 1999 - Handling Missing Data in Trees Surrogate Splits o.pdf:/Users/meghajoshi/Zotero/storage/4J7M5PWC/Feelders - 1999 - Handling Missing Data in Trees Surrogate Splits o.pdf:application/pdf}
}

@article{hahn_bayesian_2017,
	title = {Bayesian regression tree models for causal inference: regularization, confounding, and heterogeneous effects},
	shorttitle = {Bayesian regression tree models for causal inference},
	url = {http://arxiv.org/abs/1706.09523},
	abstract = {This paper develops a semi-parametric Bayesian regression model for estimating heterogeneous treatment effects from observational data. Standard nonlinear regression models, which may work quite well for prediction, can yield badly biased estimates of treatment effects when fit to data with strong confounding. Our Bayesian causal forests model avoids this problem by directly incorporating an estimate of the propensity function in the specification of the response model, implicitly inducing a covariate-dependent prior on the regression function. This new parametrization also allows treatment heterogeneity to be regularized separately from the prognostic effect of control variables, making it possible to informatively "shrink to homogeneity", in contrast to existing Bayesian non- and semi-parametric approaches.},
	language = {en},
	urldate = {2019-02-12},
	journal = {arXiv:1706.09523 [stat]},
	author = {Hahn, P. Richard and Murray, Jared S. and Carvalho, Carlos},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.09523},
	keywords = {Statistics - Methodology},
	file = {Hahn et al. - 2017 - Bayesian regression tree models for causal inferen.pdf:/Users/meghajoshi/Zotero/storage/KG2TYRL9/Hahn et al. - 2017 - Bayesian regression tree models for causal inferen.pdf:application/pdf}
}

@article{hill_bayesian_2011,
	title = {Bayesian {Nonparametric} {Modeling} for {Causal} {Inference}},
	volume = {20},
	url = {http://www.jstor.org/stable/23113385},
	language = {en},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Hill, Jennifer L.},
	year = {2011},
	pages = {217--240},
	file = {Hill - 2011 - Bayesian Nonparametric Modeling for Causal Inferen.pdf:/Users/meghajoshi/Zotero/storage/L6SRI5JS/Hill - 2011 - Bayesian Nonparametric Modeling for Causal Inferen.pdf:application/pdf}
}

@article{lee_improving_2009,
	title = {Improving propensity score weighting using machine learning},
	issn = {02776715, 10970258},
	url = {http://doi.wiley.com/10.1002/sim.3782},
	doi = {10.1002/sim.3782},
	language = {en},
	urldate = {2019-02-12},
	journal = {Statistics in Medicine},
	author = {Lee, Brian K. and Lessler, Justin and Stuart, Elizabeth A.},
	year = {2009},
	pages = {n/a--n/a},
	file = {Freedman & Berk (2008).pdf:/Users/meghajoshi/Zotero/storage/25JYTIWU/Freedman & Berk (2008).pdf:application/pdf;Lee et al. - 2009 - Improving propensity score weighting using machine.pdf:/Users/meghajoshi/Zotero/storage/98RF53CL/Lee et al. - 2009 - Improving propensity score weighting using machine.pdf:application/pdf}
}

@article{mccaffrey_propensity_2004,
	title = {Propensity {Score} {Estimation} {With} {Boosted} {Regression} for {Evaluating} {Causal} {Effects} in {Observational} {Studies}.},
	volume = {9},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.9.4.403},
	doi = {10.1037/1082-989X.9.4.403},
	abstract = {Causal effect modeling with naturalistic rather than experimental data is challenging. In observational studies participants in different treatment conditions may also differ on pretreatment characteristics that influence outcomes. Propensity score methods can theoretically eliminate these confounds for all observed covariates, but accurate estimation of propensity scores is impeded by large numbers of covariates, uncertain functional forms for their associations with treatment selection, and other problems. This paper demonstrates that boosting, a modern statistical technique, can overcome many of these obstacles. We illustrate this approach with a study of adolescent probationers in substance abuse treatment programs. Propensity score weights estimated using boosting eliminate most pretreatment group differences, and substantially alter the apparent relative effects of adolescent substance abuse treatment.},
	language = {en},
	number = {4},
	urldate = {2019-02-12},
	journal = {Psychological Methods},
	author = {McCaffrey, Daniel F. and Ridgeway, Greg and Morral, Andrew R.},
	year = {2004},
	pages = {403--425},
	file = {Cunha et al (2018).pdf:/Users/meghajoshi/Zotero/storage/NE4X9DZG/Cunha et al (2018).pdf:application/pdf;Hayes & Groner (2007).pdf:/Users/meghajoshi/Zotero/storage/V54CNRA7/Hayes & Groner (2007).pdf:application/pdf;Hong & Yu (2007).pdf:/Users/meghajoshi/Zotero/storage/C8MQHQ6E/Hong & Yu (2007).pdf:application/pdf;Jenkins et al (2016).pdf:/Users/meghajoshi/Zotero/storage/Y7MBLE2B/Jenkins et al (2016).pdf:application/pdf;Kelcey (2011).pdf:/Users/meghajoshi/Zotero/storage/LYLJ5S6E/Kelcey (2011).pdf:application/pdf;Long & Kurlaender (2009).pdf:/Users/meghajoshi/Zotero/storage/FQN9IUWP/Long & Kurlaender (2009).pdf:application/pdf;McCaffrey et al. - 2004 - Propensity Score Estimation With Boosted Regressio.pdf:/Users/meghajoshi/Zotero/storage/62W8HLNJ/McCaffrey et al. - 2004 - Propensity Score Estimation With Boosted Regressio.pdf:application/pdf;Melguizo et al (2011).pdf:/Users/meghajoshi/Zotero/storage/HCZB8B3U/Melguizo et al (2011).pdf:application/pdf;Morgan et al (2010).pdf:/Users/meghajoshi/Zotero/storage/EAFM29EQ/Morgan et al (2010).pdf:application/pdf;Park, Lubinski, Benbow (2013).pdf:/Users/meghajoshi/Zotero/storage/ZGICI4AQ/Park, Lubinski, Benbow (2013).pdf:application/pdf;Wang (2015).pdf:/Users/meghajoshi/Zotero/storage/2M5RIT49/Wang (2015).pdf:application/pdf;West et al (2014).pdf:/Users/meghajoshi/Zotero/storage/IFF2NXG6/West et al (2014).pdf:application/pdf;Wu, West, Hughes (2006).pdf:/Users/meghajoshi/Zotero/storage/HBXWRVHX/Wu, West, Hughes (2006).pdf:application/pdf;Xu & Jaggars (2011).pdf:/Users/meghajoshi/Zotero/storage/NWIZXJZD/Xu & Jaggars (2011).pdf:application/pdf;Yanovitzky et al. (2005)  .pdf:/Users/meghajoshi/Zotero/storage/Z7X6KXVM/Yanovitzky et al. (2005)  .pdf:application/pdf}
}

@article{murray_multiple_2018-1,
	title = {Multiple {Imputation}: {A} {Review} of {Practical} and {Theoretical} {Findings}},
	volume = {33},
	issn = {0883-4237},
	shorttitle = {Multiple {Imputation}},
	url = {https://projecteuclid.org/euclid.ss/1525313139},
	doi = {10.1214/18-STS644},
	abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in ﬂexible joint modeling and sequential regression/chained equations/fully conditional speciﬁcation approaches. Finally, we compare and contrast diﬀerent methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
	language = {en},
	number = {2},
	urldate = {2019-02-12},
	journal = {Statistical Science},
	author = {Murray, Jared S.},
	month = may,
	year = {2018},
	pages = {142--159},
	file = {Murray - 2018 - Multiple Imputation A Review of Practical and The.pdf:/Users/meghajoshi/Zotero/storage/CAN6U2EX/Murray - 2018 - Multiple Imputation A Review of Practical and The.pdf:application/pdf}
}

@article{murray_log-linear_2017,
	title = {Log-{Linear} {Bayesian} {Additive} {Regression} {Trees} for {Categorical} and {Count} {Responses}},
	url = {http://arxiv.org/abs/1701.01503},
	abstract = {We introduce Bayesian additive regression trees (BART) for log-linear models including multinomial logistic regression and count regression with zero-inﬂation and overdispersion. BART has been applied to nonparametric mean regression and binary classiﬁcation problems in a range of settings. However, existing applications of BART have been limited to models for Gaussian “data”, either observed or latent. This is primarily because eﬃcient MCMC algorithms are available for Gaussian likelihoods. But while many useful models are naturally cast in terms of latent Gaussian variables, many others are not – including models considered in this paper.},
	language = {en},
	urldate = {2019-02-12},
	journal = {arXiv:1701.01503 [stat]},
	author = {Murray, Jared S.},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.01503},
	keywords = {Statistics - Methodology},
	file = {Murray - 2017 - Log-Linear Bayesian Additive Regression Trees for .pdf:/Users/meghajoshi/Zotero/storage/TSUZPNGQ/Murray - 2017 - Log-Linear Bayesian Additive Regression Trees for .pdf:application/pdf}
}

@article{ridgeway_generalized_nodate,
	title = {Generalized {Boosted} {Models}: {A} guide to the gbm package},
	language = {en},
	author = {Ridgeway, Greg},
	pages = {12},
	file = {Ridgeway - Generalized Boosted Models A guide to the gbm pac.pdf:/Users/meghajoshi/Zotero/storage/UPGCIV39/Ridgeway - Generalized Boosted Models A guide to the gbm pac.pdf:application/pdf}
}

@article{roy_bayesian_2018,
	title = {Bayesian nonparametric generative models for causal inference with missing at random covariates},
	volume = {74},
	issn = {0006-341X, 1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12875},
	doi = {10.1111/biom.12875},
	abstract = {We propose a general Bayesian nonparametric (BNP) approach to causal inference in the point treatment setting. The joint distribution of the observed data (outcome, treatment, and confounders) is modeled using an enriched Dirichlet process. The combination of the observed data model and causal assumptions allows us to identify any type of causal eﬀect—diﬀerences, ratios, or quantile eﬀects, either marginally or for subpopulations of interest. The proposed BNP model is wellsuited for causal inference problems, as it does not require parametric assumptions about the distribution of confounders and naturally leads to a computationally eﬃcient Gibbs sampling algorithm. By ﬂexibly modeling the joint distribution, we are also able to impute (via data augmentation) values for missing covariates within the algorithm under an assumption of ignorable missingness, obviating the need to create separate imputed data sets. This approach for imputing the missing covariates has the additional advantage of guaranteeing congeniality between the imputation model and the analysis model, and because we use a BNP approach, parametric models are avoided for imputation. The performance of the method is assessed using simulation studies. The method is applied to data from a cohort study of human immunodeﬁciency virus/hepatitis C virus co-infected patients.},
	language = {en},
	number = {4},
	urldate = {2019-02-12},
	journal = {Biometrics},
	author = {Roy, Jason and Lum, Kirsten J. and Zeldow, Bret and Dworkin, Jordan D. and Re, Vincent Lo and Daniels, Michael J.},
	month = dec,
	year = {2018},
	pages = {1193--1202},
	file = {Austin (2009).pdf:/Users/meghajoshi/Zotero/storage/7F92TZC5/Austin (2009).pdf:application/pdf;Austin (2011).pdf:/Users/meghajoshi/Zotero/storage/59RCENLK/Austin (2011).pdf:application/pdf;Frangakis and Rubin (1999).pdf:/Users/meghajoshi/Zotero/storage/J9RRI8EC/Frangakis and Rubin (1999).pdf:application/pdf;hernanrobins_v1.10.36.pdf:/Users/meghajoshi/Zotero/storage/C93LBX2M/hernanrobins_v1.10.36.pdf:application/pdf;Ho et al. (2007).pdf:/Users/meghajoshi/Zotero/storage/8X55IM2I/Ho et al. (2007).pdf:application/pdf;Holland (1986).pdf:/Users/meghajoshi/Zotero/storage/AE24S5DA/Holland (1986).pdf:application/pdf;Roy et al. - 2018 - Bayesian nonparametric generative models for causa.pdf:/Users/meghajoshi/Zotero/storage/UQ4IWPY8/Roy et al. - 2018 - Bayesian nonparametric generative models for causa.pdf:application/pdf}
}

@book{james_introduction_2013,
	address = {New York},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning: with applications in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An introduction to statistical learning},
	language = {en},
	number = {103},
	publisher = {Springer},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	note = {OCLC: ocn828488009},
	keywords = {Mathematical models, Mathematical statistics, Problems, exercises, etc, R (Computer program language), Statistics},
	file = {James et al. - 2013 - An introduction to statistical learning with appl.pdf:/Users/meghajoshi/Zotero/storage/CSZ3Q6PE/James et al. - 2013 - An introduction to statistical learning with appl.pdf:application/pdf}
}

@article{zubizarreta_stable_2015,
	title = {Stable {Weights} that {Balance} {Covariates} for {Estimation} {With} {Incomplete} {Outcome} {Data}},
	volume = {110},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/full/10.1080/01621459.2015.1023805},
	doi = {10.1080/01621459.2015.1023805},
	language = {en},
	number = {511},
	urldate = {2019-02-12},
	journal = {Journal of the American Statistical Association},
	author = {Zubizarreta, José R.},
	month = jul,
	year = {2015},
	pages = {910--922},
	file = {Zubizarreta - 2015 - Stable Weights that Balance Covariates for Estimat.pdf:/Users/meghajoshi/Zotero/storage/LN5XGIYV/Zubizarreta - 2015 - Stable Weights that Balance Covariates for Estimat.pdf:application/pdf}
}

@article{setoguchi_evaluating_2008,
	title = {Evaluating uses of data mining techniques in propensity score estimation: a simulation study},
	volume = {17},
	issn = {10538569, 10991557},
	shorttitle = {Evaluating uses of data mining techniques in propensity score estimation},
	url = {http://doi.wiley.com/10.1002/pds.1555},
	doi = {10.1002/pds.1555},
	abstract = {Background—In propensity score modeling, it is a standard practice to optimize the prediction of exposure status based on the covariate information. In a simulation study, we examined in what situations analyses based on various types of exposure propensity score (EPS) models using data mining techniques such as recursive partitioning (RP) and neural networks (NN) produce unbiased and/or efficient results.
Method—We simulated data for a hypothetical cohort study (n=2000) with a binary exposure/ outcome and 10 binary/ continuous covariates with seven scenarios differing by non-linear and/or non-additive associations between exposure and covariates. EPS models used logistic regression (LR) (all possible main effects), RP1 (without pruning), RP2 (with pruning), and NN. We calculated c-statistics (C), standard errors (SE), and bias of exposure-effect estimates from outcome models for the PS-matched dataset.
Results—Data mining techniques yielded higher C than LR (mean: NN, 0.86; RPI, 0.79; RP2, 0.72; and LR, 0.76). SE tended to be greater in models with higher C. Overall bias was small for each strategy, although NN estimates tended to be the least biased. C was not correlated with the magnitude of bias (correlation coefficient [COR]=−0.3, p=0.1) but increased SE (COR=0.7, p{\textless}0.001).
Conclusions—Effect estimates from EPS models by simple LR were generally robust. NN models generally provided the least numerically biased estimates. C was not associated with the magnitude of bias but was with the increased SE.},
	language = {en},
	number = {6},
	urldate = {2019-02-12},
	journal = {Pharmacoepidemiology and Drug Safety},
	author = {Setoguchi, Soko and Schneeweiss, Sebastian and Brookhart, M. Alan and Glynn, Robert J. and Cook, E. Francis},
	month = jun,
	year = {2008},
	pages = {546--555},
	file = {Setoguchi et al. - 2008 - Evaluating uses of data mining techniques in prope.pdf:/Users/meghajoshi/Zotero/storage/677JLF45/Setoguchi et al. - 2008 - Evaluating uses of data mining techniques in prope.pdf:application/pdf}
}

@article{austin_balance_2009,
	title = {Balance diagnostics for comparing the distribution of baseline covariates between treatment groups in propensity-score matched samples},
	volume = {28},
	issn = {02776715},
	url = {http://doi.wiley.com/10.1002/sim.3697},
	doi = {10.1002/sim.3697},
	abstract = {The propensity score is a subject’s probability of treatment, conditional on observed baseline covariates. Conditional on the true propensity score, treated and untreated subjects have similar distributions of observed baseline covariates. Propensity-score matching is a popular method of using the propensity score in the medical literature. Using this approach, matched sets of treated and untreated subjects with similar values of the propensity score are formed. Inferences about treatment effect made using propensity-score matching are valid only if, in the matched sample, treated and untreated subjects have similar distributions of measured baseline covariates. In this paper we discuss the following methods for assessing whether the propensity score model has been correctly speciﬁed: comparing means and prevalences of baseline characteristics using standardized differences; ratios comparing the variance of continuous covariates between treated and untreated subjects; comparison of higher order moments and interactions; ﬁve-number summaries; and graphical methods such as quantile–quantile plots, side-byside boxplots, and non-parametric density plots for comparing the distribution of baseline covariates between treatment groups. We describe methods to determine the sampling distribution of the standardized difference when the true standardized difference is equal to zero, thereby allowing one to determine the range of standardized differences that are plausible with the propensity score model having been correctly speciﬁed. We highlight the limitations of some previously used methods for assessing the adequacy of the speciﬁcation of the propensity-score model. In particular, methods based on comparing the distribution of the estimated propensity score between treated and untreated subjects are uninformative. Copyright q 2009 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {25},
	urldate = {2019-02-12},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C.},
	month = nov,
	year = {2009},
	pages = {3083--3107},
	file = {Austin - 2009 - Balance diagnostics for comparing the distribution.pdf:/Users/meghajoshi/Zotero/storage/CM364ET7/Austin - 2009 - Balance diagnostics for comparing the distribution.pdf:application/pdf}
}

@article{austin_introduction_2011,
	title = {An {Introduction} to {Propensity} {Score} {Methods} for {Reducing} the {Effects} of {Confounding} in {Observational} {Studies}},
	volume = {46},
	issn = {0027-3171, 1532-7906},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00273171.2011.568786},
	doi = {10.1080/00273171.2011.568786},
	language = {en},
	number = {3},
	urldate = {2019-02-12},
	journal = {Multivariate Behavioral Research},
	author = {Austin, Peter C.},
	month = may,
	year = {2011},
	pages = {399--424},
	file = {Austin - 2011 - An Introduction to Propensity Score Methods for Re.pdf:/Users/meghajoshi/Zotero/storage/CBVVKSN9/Austin - 2011 - An Introduction to Propensity Score Methods for Re.pdf:application/pdf}
}

@article{frangakis_addressing_1999-2,
	title = {Addressing complications of intention-to-treat analysis in the combined presence of all-or-none treatment-noncompliance and subsequent missing outcomes},
	volume = {86},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/86.2.365},
	doi = {10.1093/biomet/86.2.365},
	abstract = {We study the combined impact that all-or-none compliance and subsequent missing outcomes can have on the estimation of the intention-to-treat effect of assignment in randomised studies. In this setting, a standard analysis, which drops subjects with missing outcomes and ignores compliance information, can be biased for the intention-to-treat effect. To address all-or-none compliance that is followed by missing outcomes, we construct a new estimation procedure for the intention-to-treat effect that maintains good randomisation-based properties under more plausible, nonignorable noncompliance and nonignorable missing-outcome conditions: the 'compound exclusion restriction' on the effect of assignment and the 'latent ignorability' of the missing data mechanism. We present both theoretical results and a simulation study. Moreover, we show how the two key concepts of compound exclusion and latent ignorability are relevant in more complicated settings, such as right censoring of a time-to-event outcome.},
	language = {en},
	number = {2},
	urldate = {2019-02-12},
	journal = {Biometrika},
	author = {Frangakis, C.},
	month = jun,
	year = {1999},
	pages = {365--379},
	file = {Frangakis - 1999 - Addressing complications of intention-to-treat ana.pdf:/Users/meghajoshi/Zotero/storage/B3RBICJI/Frangakis - 1999 - Addressing complications of intention-to-treat ana.pdf:application/pdf}
}

@article{ho_matching_2007,
	title = {Matching as {Nonparametric} {Preprocessing} for {Reducing} {Model} {Dependence} in {Parametric} {Causal} {Inference}},
	volume = {15},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/product/identifier/S1047198700006483/type/journal_article},
	doi = {10.1093/pan/mpl013},
	language = {en},
	number = {03},
	urldate = {2019-02-12},
	journal = {Political Analysis},
	author = {Ho, Daniel E. and Imai, Kosuke and King, Gary and Stuart, Elizabeth A.},
	year = {2007},
	pages = {199--236},
	file = {Ho et al. - 2007 - Matching as Nonparametric Preprocessing for Reduci.pdf:/Users/meghajoshi/Zotero/storage/PRZHFX39/Ho et al. - 2007 - Matching as Nonparametric Preprocessing for Reduci.pdf:application/pdf}
}

@article{holland_statistics_1986,
	title = {Statistics and {Causal} {Inference}},
	volume = {81},
	issn = {01621459},
	url = {https://www.jstor.org/stable/2289064?origin=crossref},
	doi = {10.2307/2289064},
	language = {en},
	number = {396},
	urldate = {2019-02-12},
	journal = {Journal of the American Statistical Association},
	author = {Holland, Paul W.},
	month = dec,
	year = {1986},
	pages = {945},
	file = {Holland - 1986 - Statistics and Causal Inference.pdf:/Users/meghajoshi/Zotero/storage/EVKTC3LG/Holland - 1986 - Statistics and Causal Inference.pdf:application/pdf}
}

@article{rickles_using_2011,
	title = {Using {Interviews} to {Understand} the {Assignment} {Mechanism} in a {Nonexperimental} {Study}: {The} {Case} of {Eighth} {Grade} {Algebra}},
	volume = {35},
	issn = {0193-841X, 1552-3926},
	shorttitle = {Using {Interviews} to {Understand} the {Assignment} {Mechanism} in a {Nonexperimental} {Study}},
	url = {http://journals.sagepub.com/doi/10.1177/0193841X11428644},
	doi = {10.1177/0193841X11428644},
	language = {en},
	number = {5},
	urldate = {2019-02-12},
	journal = {Evaluation Review},
	author = {Rickles, Jordan H.},
	month = oct,
	year = {2011},
	pages = {490--522},
	file = {Rickles - 2011 - Using Interviews to Understand the Assignment Mech.pdf:/Users/meghajoshi/Zotero/storage/HGESBQMN/Rickles - 2011 - Using Interviews to Understand the Assignment Mech.pdf:application/pdf}
}

@article{rosenbaum_central_1983-2,
	title = {The {Central} {Role} of the {Propensity} {Score} in {Observational} {Studies} for {Causal} {Effects}},
	volume = {70},
	issn = {00063444},
	url = {https://www.jstor.org/stable/2335942?origin=crossref},
	doi = {10.2307/2335942},
	number = {1},
	urldate = {2019-02-12},
	journal = {Biometrika},
	author = {Rosenbaum, Paul R. and Rubin, Donald B.},
	month = apr,
	year = {1983},
	pages = {41},
	file = {Rosenbaum and Rubin - 1983 - The Central Role of the Propensity Score in Observ.pdf:/Users/meghajoshi/Zotero/storage/RLMAQA3L/Rosenbaum and Rubin - 1983 - The Central Role of the Propensity Score in Observ.pdf:application/pdf}
}

@book{rubin_matched_2006,
	address = {Cambridge},
	title = {Matched {Sampling} for {Causal} {Effects}},
	isbn = {978-0-511-81072-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511810725},
	abstract = {Propensity score methodology can be used to help design observational studies in a way analogous to the way randomized experiments are designed: without seeing any answers involving outcome variables. The typical models used to analyze observational data (e.g., least squares regressions, difference of difference methods) involve outcomes, and so cannot be used for design in this sense. Because the propensity score is a function only of covariates, not outcomes, repeated analyses attempting to balance covariate distributions across treatment groups do not bias estimates of the treatment effect on outcome variables. This theme will the primary focus of this article: how to use the techniques of matching, subclassiﬁcation and=or weighting to help design observational studies. The article also proposes a new diagnostic table to aid in this endeavor, which is especially useful when there are many covariates under consideration. The conclusion of the initial design phase may be that the treatment and control groups are too far apart to produce reliable effect estimates without heroic modeling assumptions. In such cases, it may be wisest to abandon the intended observational study, and search for a more acceptable data set where such heroic modeling assumptions are not necessary. The ideas and techniques will be illustrated using the initial design of an observational study for use in the tobacco litigation based on the NMES data set.},
	language = {en},
	urldate = {2019-02-12},
	publisher = {Cambridge University Press},
	author = {Rubin, Donald B.},
	year = {2006},
	doi = {10.1017/CBO9780511810725},
	file = {Rubin - 2006 - Matched Sampling for Causal Effects.pdf:/Users/meghajoshi/Zotero/storage/IJTAYVRL/Rubin - 2006 - Matched Sampling for Causal Effects.pdf:application/pdf}
}

@book{shadish_experimental_2001,
	address = {Boston},
	title = {Experimental and quasi-experimental designs for generalized causal inference},
	isbn = {978-0-395-61556-0},
	language = {en},
	publisher = {Houghton Mifflin},
	author = {Shadish, William R. and Cook, Thomas D. and Campbell, Donald T.},
	year = {2001},
	keywords = {Causation, Experiments},
	file = {Shadish et al. - 2001 - Experimental and quasi-experimental designs for ge.pdf:/Users/meghajoshi/Zotero/storage/TYFGJMPG/Shadish et al. - 2001 - Experimental and quasi-experimental designs for ge.pdf:application/pdf}
}

@article{stuart_developing_2008,
	title = {Developing practical recommendations for the use of propensity scores: {Discussion} of ‘{A} critical appraisal of propensity score matching in the medical literature between 1996 and 2003’ by {Peter} {Austin},{Statistics} in {Medicine}},
	volume = {27},
	issn = {02776715, 10970258},
	shorttitle = {Developing practical recommendations for the use of propensity scores},
	url = {http://doi.wiley.com/10.1002/sim.3207},
	doi = {10.1002/sim.3207},
	language = {en},
	number = {12},
	urldate = {2019-02-12},
	journal = {Statistics in Medicine},
	author = {Stuart, Elizabeth A.},
	month = may,
	year = {2008},
	pages = {2062--2065},
	file = {Stuart - 2008 - Developing practical recommendations for the use o.pdf:/Users/meghajoshi/Zotero/storage/GENWGK9F/Stuart - 2008 - Developing practical recommendations for the use o.pdf:application/pdf}
}

@article{stuart_matching_2010-2,
	title = {Matching {Methods} for {Causal} {Inference}: {A} {Review} and a {Look} {Forward}},
	volume = {25},
	issn = {0883-4237},
	shorttitle = {Matching {Methods} for {Causal} {Inference}},
	url = {http://projecteuclid.org/euclid.ss/1280841730},
	doi = {10.1214/09-STS313},
	abstract = {When estimating causal effects using observational data, it is desirable to replicate a randomized experiment as closely as possible by obtaining treated and control groups with similar covariate distributions. This goal can often be achieved by choosing well-matched samples of the original treated and control groups, thereby reducing bias due to the covariates. Since the 1970’s, work on matching methods has examined how to best choose treated and control subjects for comparison. Matching methods are gaining popularity in fields such as economics, epidemiology, medicine, and political science. However, until now the literature and related advice has been scattered across disciplines. Researchers who are interested in using matching methods–or developing methods related to matching–do not have a single place to turn to learn about past and current research. This paper provides a structure for thinking about matching methods and guidance on their use, coalescing the existing research (both old and new) and providing a summary of where the literature on matching methods is now and where it should be headed.},
	language = {en},
	number = {1},
	urldate = {2019-02-12},
	journal = {Statistical Science},
	author = {Stuart, Elizabeth A.},
	month = feb,
	year = {2010},
	pages = {1--21},
	file = {Stuart - 2010 - Matching Methods for Causal Inference A Review an.pdf:/Users/meghajoshi/Zotero/storage/MGRKKUY3/Stuart - 2010 - Matching Methods for Causal Inference A Review an.pdf:application/pdf}
}

@article{thoemmes_graphical_2015,
	title = {Graphical {Representation} of {Missing} {Data} {Problems}},
	volume = {22},
	issn = {1070-5511, 1532-8007},
	url = {http://www.tandfonline.com/doi/full/10.1080/10705511.2014.937378},
	doi = {10.1080/10705511.2014.937378},
	abstract = {Rubin’s classic missingness mechanisms are central to handling missing data and minimizing biases that can arise due to missingness. However, the formulaic expressions that posit certain independencies among missing and observed data are difﬁcult to grasp. As a result, applied researchers often rely on informal translations of these assumptions. We present a graphical representation of missing data mechanism, formalized in Mohan, Pearl, and Tian (2013). We show that graphical models provide a tool for comprehending, encoding, and communicating assumptions about the missingness process. Furthermore, we demonstrate on several examples how graph-theoretical criteria can determine if biases due to missing data might emerge in some estimates of interests and which auxiliary variables are needed to control for such biases, given assumptions about the missingness process.},
	language = {en},
	number = {4},
	urldate = {2019-02-12},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Thoemmes, Felix and Mohan, Karthika},
	month = oct,
	year = {2015},
	pages = {631--642},
	file = {Thoemmes and Mohan - 2015 - Graphical Representation of Missing Data Problems.pdf:/Users/meghajoshi/Zotero/storage/P8RZ64SQ/Thoemmes and Mohan - 2015 - Graphical Representation of Missing Data Problems.pdf:application/pdf}
}

@article{freedman_weighting_2008,
	title = {Weighting {Regressions} by {Propensity} {Scores}},
	volume = {32},
	issn = {0193-841X, 1552-3926},
	url = {http://journals.sagepub.com/doi/10.1177/0193841X08317586},
	doi = {10.1177/0193841X08317586},
	language = {en},
	number = {4},
	urldate = {2019-02-12},
	journal = {Evaluation Review},
	author = {Freedman, David A. and Berk, Richard A.},
	month = aug,
	year = {2008},
	pages = {392--409},
	file = {Freedman and Berk - 2008 - Weighting Regressions by Propensity Scores.pdf:/Users/meghajoshi/Zotero/storage/9M5E8EGA/Freedman and Berk - 2008 - Weighting Regressions by Propensity Scores.pdf:application/pdf;Friedman (2002).pdf:/Users/meghajoshi/Zotero/storage/TBJD7DZD/Friedman (2002).pdf:application/pdf}
}





@book{imbens_causal_2015,
	address = {New York},
	title = {Causal inference for statistics, social, and biomedical sciences: an introduction},
	isbn = {978-0-521-88588-1},
	shorttitle = {Causal inference for statistics, social, and biomedical sciences},
	publisher = {Cambridge University Press},
	author = {Imbens, Guido and Rubin, Donald B.},
	year = {2015},
	keywords = {Causation, Inference, Research, Social sciences}
}

@book{gerber_field_2012,
	address = {New York},
	edition = {1st ed},
	title = {Field experiments: design, analysis, and interpretation},
	isbn = {978-0-393-97995-4},
	shorttitle = {Field experiments},
	publisher = {W. W. Norton},
	author = {Gerber, Alan S. and Green, Donald P.},
	year = {2012},
	keywords = {Social sciences, Political science, Research Methodology, Study and teaching (Higher)},
	annote = {Introduction -- Causal inference and experimentation -- Sampling distributions, statistical inference, and hypothesis testing -- Using covariates in experimental design and analysis -- One-sided noncompliance -- Two-sided noncompliance -- Attrition -- Interference between experimental units -- Mediation -- Integration of research findings -- Writing an experimental prospectus, research report, and journal article -- Experimental challenges and opportunities}
}





@book{james_introduction_2013-1,
	address = {New York},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning: with applications in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An introduction to statistical learning},
	number = {103},
	publisher = {Springer},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	note = {OCLC: ocn828488009},
	keywords = {Mathematical models, Mathematical statistics, Problems, exercises, etc, R (Computer program language), Statistics},
	annote = {Includes index}
}

@article{rosenbaum_reducing_1984,
	title = {Reducing {Bias} in {Observational} {Studies} {Using} {Subclassification} on the {Propensity} {Score}},
	volume = {79},
	issn = {01621459},
	url = {https://www.jstor.org/stable/2288398?origin=crossref},
	doi = {10.2307/2288398},
	number = {387},
	urldate = {2019-02-15},
	journal = {Journal of the American Statistical Association},
	author = {Rosenbaum, Paul R. and Rubin, Donald B.},
	month = sep,
	year = {1984},
	pages = {516}
}



@article{rubin_inference_1976,
	title = {Inference and {Missing} {Data}},
	volume = {63},
	issn = {00063444},
	url = {https://www.jstor.org/stable/2335739?origin=crossref},
	doi = {10.2307/2335739},
	number = {3},
	urldate = {2019-02-15},
	journal = {Biometrika},
	author = {Rubin, Donald B.},
	month = dec,
	year = {1976},
	pages = {581}
}



@article{friedman_stochastic_2002,
	title = {Stochastic gradient boosting},
	volume = {38},
	issn = {01679473},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947301000652},
	doi = {10.1016/S0167-9473(01)00065-2},
	abstract = {Gradient boosting constructs additive regression models by sequentially ÿtting a simple parameterized function (base learner) to current “pseudo”-residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Speciÿcally, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to ÿt the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner. c 2002 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {4},
	urldate = {2019-02-15},
	journal = {Computational Statistics \& Data Analysis},
	author = {Friedman, Jerome H.},
	month = feb,
	year = {2002},
	pages = {367--378},
	file = {Friedman - 2002 - Stochastic gradient boosting.pdf:/Users/meghajoshi/Box Sync/QP Megha/articles machine learning/Friedman - 2002 - Stochastic gradient boosting.pdf:application/pdf}
}

@book{rubin_multiple_1987,
	address = {New York},
	series = {Wiley series in probability and mathematical statistics},
	title = {Multiple imputation for nonresponse in surveys},
	isbn = {978-0-471-08705-2},
	publisher = {Wiley},
	author = {Rubin, Donald B.},
	year = {1987},
	keywords = {Multiple imputation (Statistics), Nonresponse (Statistics), Response rate, Social surveys},
	annote = {Includes index}
}

@unpublished{hill_2004,
    author  = "Jennifer Hill",
    title   = "Reducing bias in treatment effect estimation in observational studies suffering from missing data",
    institution = "Columbia University Institute for Social & Economic Research & Policy (ISERP)",
    year    = "2004",
    note    = "Working paper",
}

@unpublished{jp_2019,
    author  = "James E. Pustejovsky and Megha Joshi",
    title   = "Evaluating the Transition to College Mathematics Course in Texas High Schools: Findings
from the First Year of Implementation",
    institution = "Greater Texas Foundation",
    year    = "2019",
    note    = "White paper",
}


@unpublished{hernan_2019,
    author  = "Hernán, M. A. and Robins, J. M.",
    title   = "Causal Inference",
    publisher = "Chapman & Hall/CRC, forthcoming",
    address = "Boca Raton",
    year    = "2019",
    note    = "Forthcoming",
}


@article{neyman_application_1923,
	title = {On the {Application} of {Probability} {Theory} to {Agricultural} {Experiments}. {Essay} on {Principles}. {Section} 9},
	volume = {5},
	number = {4},
	journal = {Statistical Science},
	author = {Neyman, Jerzy},
	month = nov,
	year = {1923},
	pages = {465--472},
}


@article{wu_effect_2008,
	title = {Effect of {Retention} in {First} {Grade} on {Children}'s {Achievement} {Trajectories} {Over} 4 {Years}: {A} {Piecewise} {Growth} {Analysis} {Using} {Propensity} {Score} {Matching}},
	volume = {100},
	issn = {0022-0663},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/19337582},
	abstract = {The authors investigated the relatively short-term and longer term effects of grade retention in 1st grade on the growth of mathematics and reading achievement over 4 years. The authors initially identified a large multiethnic sample (n = 784) of children who were below the median in literacy at school entrance. From this sample, the authors closely matched 1 retained with 1 promoted child (n = 97 pairs) on the basis of propensity scores constructed from 72 background variables and compared growth of retained and promoted children using Rasch-modeled W scores and grade standard scores, which facilitate age-based and grade-based comparisons, respectively. When using W scores, retained children experienced a slower increase in both mathematics and reading achievement in the short term but a faster increase in reading achievement in the longer term than did the promoted children. When using grade standard scores, retained children experienced a faster increase in the short term but a faster decrease in the longer term in both mathematics and reading achievement than did promoted children. Some of the retention effects were moderated by limited English language proficiency, home-school relationship, and children's externalizing problems.},
	number = {4},
	journal = {Journal of educational psychology},
	author = {Wu, Wei and West, Stephen G and Hughes, Jan N},
	month = nov,
	year = {2008},
	pages = {727--740}
}


@article{Levin2008remediation,
  title = {Remediation in the {{Community College}}: {{An Evaluator}}'s {{Perspective}}},
  volume = {35},
  issn = {0091-5521},
  doi = {10.1177/0091552107310118},
  number = {3},
  journal = {Community College Review},
  author = {Levin, H. M. and Calcagno, Juan Carlos},
  year = {2008},
  keywords = {1 large numbers of,community colleges,education in recent times,has surely become one,he,of the most controver-,program evaluation,remedial education,remediation crisis,sial issues in higher},
  pages = {181-207}
}

@unpublished{Calcagno2008,
  title = {The {{Impact}} of {{Postsecondary Remediation Using}} a {{Regression Discontinuity Approach}}: {{Addressing Endogenous Sorting}} and {{Noncompliance}}},
  abstract = {Remedial or developmental courses are the most common instruments used to assist postsecondary students who are not ready for college-level coursework. However, despite its important role in higher education and substantial costs, there is little rigorous evidence on the effectiveness of college remediation on the outcomes of students. This study uses a detailed dataset to identify the causal effect of remediation on the outcomes of nearly 100,000 college students in Florida. Using a Regression Discontinuity design, we provide causal estimates while also investigating possible endogenous sorting around the policy cutoff. The results suggest math and reading remedial courses have mixed benefits. Being assigned to remediation appears to increase persistence to the second year and the total number of credits completed for students on the margin of passing out of the requirement, but it does not increase the completion of college-level credits or eventual degree completion. Taken together, the results suggest that remediation might promote early persistence in college, but it does not necessarily help students on the margin of passing the placement cutoff make long-term progress toward earning a degree.},
  author = {Calcagno, Juan Carlos and Long, Bridget Terry},
  year = {2008},
  keywords = {The Impact of Postsecondary Remediation Using a Re},
  doi = {10.3386/w14194},
  booktitle = {National {{Bureau}} of {{Economic Research Working Paper Series}}},
  number = {April},
  volume = {No. 14194},
  pmid = {15391716}
}

@Article{attewell2006NewEvidenceCollege,
  langid = {english},
  title = {New {{Evidence}} on {{College Remediation}}},
  volume = {77},
  issn = {1538-4640},
  url = {http://muse.jhu.edu/content/crossref/journals/journal_of_higher_education/v077/77.5attewell.html},
  doi = {10.1353/jhe.2006.0037},
  number = {5},
  journaltitle = {The Journal of Higher Education},
  urldate = {2019-01-04},
  date = {2006},
  pages = {886-924},
  author = {Paul A. Attewell and David E. Lavin and Thurston Domina and Tania Levey},
}

@article{Scott-Clayton2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.1669v3},
  title = {Development, Discouragement, or Diversion? {{New}} Evidence on the Effects of College Remediation Policy},
  volume = {10},
  issn = {1557-3060},
  doi = {10.1162/EDFP_a_00150},
  number = {1},
  journal = {Education Finance and Policy},
  author = {{Scott-Clayton}, Judith and Rodriguez, Olga},
  month = jan,
  year = {2015},
  keywords = {icle},
  pages = {4-45},
  isbn = {9788578110796},
  arxivid = {arXiv:1011.1669v3},
  pmid = {25246403}
}

@article{rubin_estimating_1974,
	title = {Estimating causal effects of treatments in randomized and nonrandomized studies.},
	volume = {66},
	issn = {0022-0663},
	url = {http://content.apa.org/journals/edu/66/5/688},
	doi = {10.1037/h0037350},
	language = {en},
	number = {5},
	urldate = {2019-03-08},
	journal = {Journal of Educational Psychology},
	author = {Rubin, Donald B.},
	year = {1974},
	pages = {688--701}
}

@article{schafer_average_2008,
	title = {Average causal effects from nonrandomized studies: {A} practical guide and simulated example.},
	volume = {13},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Average causal effects from nonrandomized studies},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0014268},
	doi = {10.1037/a0014268},
	language = {en},
	number = {4},
	urldate = {2019-03-08},
	journal = {Psychological Methods},
	author = {Schafer, Joseph L. and Kang, Joseph},
	year = {2008},
	pages = {279--313}
}

@article{cole_constructing_2008,
	title = {Constructing {Inverse} {Probability} {Weights} for {Marginal} {Structural} {Models}},
	volume = {168},
	issn = {0002-9262, 1476-6256},
	url = {https://academic.oup.com/aje/article-lookup/doi/10.1093/aje/kwn164},
	doi = {10.1093/aje/kwn164},
	language = {en},
	number = {6},
	urldate = {2019-03-08},
	journal = {American Journal of Epidemiology},
	author = {Cole, S. R. and Hernán, M. A.},
	month = jul,
	year = {2008},
	pages = {656--664},
	file = {Full Text:/Users/meghajoshi/Zotero/storage/S9SW38DN/Cole and Hernan - 2008 - Constructing Inverse Probability Weights for Margi.pdf:application/pdf}
}


@article{austin_variance_2016,
	title = {Variance estimation when using inverse probability of treatment weighting ({IPTW}) with survival analysis: {Variance} estimation for {IPTW} with survival outcomes},
	volume = {35},
	issn = {02776715},
	shorttitle = {Variance estimation when using inverse probability of treatment weighting ({IPTW}) with survival analysis},
	url = {http://doi.wiley.com/10.1002/sim.7084},
	doi = {10.1002/sim.7084},
	language = {en},
	number = {30},
	urldate = {2019-03-08},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C.},
	month = dec,
	year = {2016},
	pages = {5642--5655},
	file = {Full Text:/Users/meghajoshi/Zotero/storage/7DYJJD2R/Austin - 2016 - Variance estimation when using inverse probability.pdf:application/pdf}
}

@article{little2008selection,
  title={Selection and pattern-mixture models},
  author={Little, Roderick JA},
  journal={Longitudinal data analysis},
  pages={409--431},
  year={2008},
  publisher={Chapman and Hall/CRC Press Boca Raton}
}


@article{lee_weight_2011,
	title = {Weight {Trimming} and {Propensity} {Score} {Weighting}},
	volume = {6},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0018174},
	doi = {10.1371/journal.pone.0018174},
	abstract = {Propensity score weighting is sensitive to model misspecification and outlying weights that can unduly influence results. The authors investigated whether trimming large weights downward can improve the performance of propensity score weighting and whether the benefits of trimming differ by propensity score estimation method. In a simulation study, the authors examined the performance of weight trimming following logistic regression, classification and regression trees (CART), boosted CART, and random forests to estimate propensity score weights. Results indicate that although misspecified logistic regression propensity score models yield increased bias and standard errors, weight trimming following logistic regression can improve the accuracy and precision of final parameter estimates. In contrast, weight trimming did not improve the performance of boosted CART and random forests. The performance of boosted CART and random forests without weight trimming was similar to the best performance obtainable by weight trimmed logistic regression estimated propensity scores. While trimming may be used to optimize propensity score weights estimated using logistic regression, the optimal level of trimming is difficult to determine. These results indicate that although trimming can improve inferences in some settings, in order to consistently improve the performance of propensity score weighting, analysts should focus on the procedures leading to the generation of weights (i.e., proper specification of the propensity score model) rather than relying on ad-hoc methods such as weight trimming.},
	language = {en},
	number = {3},
	urldate = {2019-04-20},
	journal = {PLoS ONE},
	author = {Lee, Brian K. and Lessler, Justin and Stuart, Elizabeth A.},
	editor = {Biondi-Zoccai, Giuseppe},
	month = mar,
	year = {2011},
	pages = {e18174},
	file = {Lee, Lessler, Stuart (2011).PDF:/Users/meghajoshi/Box Sync/QP Megha/articles machine learning/Lee, Lessler, Stuart (2011).PDF:application/pdf}
}

@article{austin_moving_2015,
	title = {Moving towards best practice when using inverse probability of treatment weighting ({IPTW}) using the propensity score to estimate causal treatment effects in observational studies},
	volume = {34},
	issn = {02776715},
	url = {http://doi.wiley.com/10.1002/sim.6607},
	doi = {10.1002/sim.6607},
	abstract = {The propensity score is defined as a subject’s probability of treatment selection, conditional on observed baseline covariates. Weighting subjects by the inverse probability of treatment received creates a synthetic sample in which treatment assignment is independent of measured baseline covariates. Inverse probability of treatment weighting (IPTW) using the propensity score allows one to obtain unbiased estimates of average treatment effects. However, these estimates are only valid if there are no residual systematic differences in observed baseline characteristics between treated and control subjects in the sample weighted by the estimated inverse probability of treatment. We report on a systematic literature review, in which we found that the use of IPTW has increased rapidly in recent years, but that in the most recent year, a majority of studies did not formally examine whether weighting balanced measured covariates between treatment groups. We then proceed to describe a suite of quantitative and qualitative methods that allow one to assess whether measured baseline covariates are balanced between treatment groups in the weighted sample. The quantitative methods use the weighted standardized difference to compare means, prevalences, higher-order moments, and interactions. The qualitative methods employ graphical methods to compare the distribution of continuous baseline covariates between treated and control subjects in the weighted sample. Finally, we illustrate the application of these methods in an empirical case study. We propose a formal set of balance diagnostics that contribute towards an evolving concept of ‘best practice’ when using IPTW to estimate causal treatment effects using observational data © 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
	language = {en},
	number = {28},
	urldate = {2019-04-20},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C. and Stuart, Elizabeth A.},
	month = dec,
	year = {2015},
	pages = {3661--3679},
	file = {Austin & Stuart (2015).pdf:/Users/meghajoshi/Box Sync/QP Megha/causal inference/IPW/Austin & Stuart (2015).pdf:application/pdf}
}


@Manual{R_cite,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019},
    url = {https://www.R-project.org/},
  }

@Manual{twang,
    title = {twang: Toolkit for Weighting and Analysis of Nonequivalent Groups},
    author = {Greg Ridgeway and Daniel F. McCaffrey and Andrew Morral and Beth Ann Griffin and Lane Burgette},
    year = {2017},
    note = {R package version 1.5},
    url = {https://CRAN.R-project.org/package=twang},
  }

@Article{vanB_2011,
    title = {{mice}: Multivariate Imputation by Chained Equations in R},
    author = {Stef {van Buuren} and Karin Groothuis-Oudshoorn},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {45},
    number = {3},
    pages = {1--67},
    url = {http://www.jstatsoft.org/v45/i03/},
  }
  
  
@book{van2018flexible,
  title={Flexible imputation of missing data},
  author={{van Buuren}, Stef},
  year={2018},
  publisher={Chapman and Hall/CRC}
}


@article{reardon2009effect,
  title={The effect of Catholic schooling on math and reading development in kindergarten through fifth grade},
  author={Reardon, Sean F and Cheadle, Jacob E and Robinson, Joseph P},
  journal={Journal of Research on Educational Effectiveness},
  volume={2},
  number={1},
  pages={45--87},
  year={2009},
  publisher={Taylor \& Francis}
}


@book{schafer1997analysis,
  title={Analysis of incomplete multivariate data},
  author={Schafer, Joseph L},
  year={1997},
  publisher={Chapman and Hall/CRC}
}

@article{dugoff2014generalizing,
  title={Generalizing observational study results: applying propensity score methods to complex surveys},
  author={DuGoff, Eva H and Schuler, Megan and Stuart, Elizabeth A},
  journal={Health services research},
  volume={49},
  number={1},
  pages={284--303},
  year={2014},
  publisher={Wiley Online Library}
}

@article{austin2018propensity,
  title={Propensity score matching and complex surveys},
  author={Austin, Peter C and Jembere, Nathaniel and Chiu, Maria},
  journal={Statistical methods in medical research},
  volume={27},
  number={4},
  pages={1240--1257},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}


@book{little2019statistical,
  title={Statistical analysis with missing data},
  author={Little, Roderick JA and Rubin, Donald B},
  volume={793},
  year={2019},
  publisher={Wiley}
}


@article{akande2017empirical,
  title={An empirical comparison of multiple imputation methods for categorical data},
  author={Akande, Olanrewaju and Li, Fan and Reiter, Jerome},
  journal={The American Statistician},
  volume={71},
  number={2},
  pages={162--170},
  year={2017},
  publisher={Taylor \& Francis}
}


@article{schouten2018generating,
  title={Generating missing values for simulation purposes: a multivariate amputation procedure},
  author={Schouten, Rianne Margaretha and Lugtig, Peter and Vink, Gerko},
  journal={Journal of Statistical Computation and Simulation},
  volume={88},
  number={15},
  pages={2909--2930},
  year={2018},
  publisher={Taylor \& Francis}
}

@book{brand1999development,
  title={Development, implementation and evaluation of multiple imputation strategies for the statistical analysis of incomplete data sets},
  author={Brand, Jaap JPL},
  year={1999}
}

@article{kang2007demystifying,
  title={Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data},
  author={Kang, Joseph DY and Schafer, Joseph L},
  journal={Statistical science},
  volume={22},
  number={4},
  pages={523--539},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}


@misc{ejdemyr_2018, 
title={ECLS}, 
author={Ejdemyr, Simon}, 
url={https://github.com/sejdemyr/ecls}, 
journal={GitHub}, 
year={2018}, 
}

@article{im2016effects,
  title={Effects of extracurricular participation during middle school on academic motivation and achievement at grade 9},
  author={Im, Myung Hee and Hughes, Jan N and Cao, Qian and Kwok, Oi-man},
  journal={American educational research journal},
  volume={53},
  number={5},
  pages={1343--1375},
  year={2016},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{tourangeau2009early,
  title={Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K): Combined User's Manual for the ECLS-K Eighth-Grade and K-8 Full Sample Data Files and Electronic Codebooks. NCES 2009-004.},
  author={Tourangeau, Karen and Nord, Christine and L{\^e}, Thanh and Sorongon, Alberto G and Najarian, Michelle},
  journal={National Center for Education Statistics},
  year={2009},
  publisher={ERIC}
}

@article{imbens2004nonparametric,
  title={Nonparametric estimation of average treatment effects under exogeneity: A review},
  author={Imbens, Guido W},
  journal={Review of Economics and statistics},
  volume={86},
  number={1},
  pages={4--29},
  year={2004},
  publisher={MIT Press}
}


@article{ali2014propensity,
  title={Propensity score balance measures in pharmacoepidemiology: a simulation study},
  author={Ali, M Sanni and Groenwold, Rolf HH and Pestman, Wiebe R and Belitser, Svetlana V and Roes, Kit CB and Hoes, Arno W and de Boer, Anthonius and Klungel, Olaf H},
  journal={Pharmacoepidemiology and drug safety},
  volume={23},
  number={8},
  pages={802--811},
  year={2014},
  publisher={Wiley Online Library}
}

@article{dorie2019automated,
  title={Automated versus do-it-yourself methods for causal inference: Lessons learned from a data analysis competition},
  author={Dorie, Vincent and Hill, Jennifer and Shalit, Uri and Scott, Marc and Cervone, Dan and others},
  journal={Statistical Science},
  volume={34},
  number={1},
  pages={43--68},
  year={2019},
  publisher={Institute of Mathematical Statistics}
}

@article{rosenbaum1986dropping,
  title={Dropping out of high school in the United States: An observational study},
  author={Rosenbaum, Paul R},
  journal={Journal of Educational Statistics},
  volume={11},
  number={3},
  pages={207--224},
  year={1986},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}


@Article{tidy_2020,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686},
  }
  
@Manual{estimatr_2020,
    title = {estimatr: Fast Estimators for Design-Based Inference},
    author = {Graeme Blair and Jasper Cooper and Alexander Coppock and Macartan Humphreys and Luke Sonnet},
    year = {2020},
    note = {R package version 0.22.0},
    url = {https://CRAN.R-project.org/package=estimatr},
  }
  
@Manual{naniar_2020,
    title = {naniar: Data Structures, Summaries, and Visualisations for Missing Data},
    author = {Nicholas Tierney and Di Cook and Miles McBain and Colin Fay},
    year = {2020},
    note = {R package version 0.5.1},
    url = {https://CRAN.R-project.org/package=naniar},
  }
  
@Article{mice_2011,
    title = {{mice}: Multivariate Imputation by Chained Equations in R},
    author = {Stef {van Buuren} and Karin Groothuis-Oudshoorn},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {45},
    number = {3},
    pages = {1-67},
    url = {https://www.jstatsoft.org/v45/i03/},
  }

@Manual{broom_2020,
    title = {broom: Convert Statistical Analysis Objects into Tidy Tibbles},
    author = {David Robinson and Alex Hayes},
    year = {2020},
    note = {R package version 0.5.6},
    url = {https://CRAN.R-project.org/package=broom},
  }
  
@Manual{matchthem_2020,
    title = {MatchThem: Matching and Weighting Multiply Imputed Datasets},
    author = {Farhad Pishgar and Noah Greifer},
    year = {2020},
    note = {R package version 0.9.3},
    url = {https://CRAN.R-project.org/package=MatchThem},
  }