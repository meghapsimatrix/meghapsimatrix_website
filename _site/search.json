[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Individual Participant Data Meta-Analysis: Example with R\n\n\n\n\n\n\nmeta-analyis\n\n\nheterogeneity\n\n\nipdma\n\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\nMegha Joshi\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Should I Use to Quantify Heterogeneity in Meta-Analysis?\n\n\n\n\n\n\nmeta-analyis\n\n\nheterogeneity\n\n\nI squared\n\n\ntau squared\n\n\nMichael Borenstein\n\n\n\n\n\n\n\n\n\nMay 17, 2021\n\n\nMegha Joshi\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is a Confidence Interval?\n\n\n\n\n\n\nstatistics\n\n\nconfidence intervals\n\n\n\n\n\n\n\n\n\nMay 11, 2021\n\n\nMegha Joshi\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity Score Analysis with Multiply Imputed Data\n\n\n\n\n\n\npropensity score\n\n\nmissing data\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\nJun 23, 2020\n\n\nMegha Joshi\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Data in Propensity Score Analysis\n\n\n\n\n\n\npropensity score\n\n\nmissing data\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\nApr 16, 2020\n\n\nMegha Joshi\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous Treatment in Propensity Score Analysis\n\n\n\n\n\n\npropensity score analysis\n\n\ncausal inference\n\n\ncontinuous treatment\n\n\n\n\n\n\n\n\n\nOct 19, 2019\n\n\nMegha Joshi\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Selected Publications",
    "section": "",
    "text": "Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2022\n\n\nMegha Joshi, James E. Pustejovsky, S. Natasha Beretvas\n\n\n\n\n\n\n\n\n\n\n\n\nThe performance of multivariate methods for two-group comparisons with small samples and incomplete data\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2019\n\n\nKeenan A. Pituch, Megha Joshi, Molly E Cain, Tiffany A Whittaker, Wanchen Chang, Ryoungsun Park, Graham J McDougall\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "software/simhelpers/index.html",
    "href": "software/simhelpers/index.html",
    "title": "simhelpers",
    "section": "",
    "text": "Monte Carlo simulations are computer experiments designed to study the performance of statistical methods under known data-generating conditions (Morris, White, & Crowther, 2019). Methodologists use simulations to examine questions such as: (1) how does ordinary least squares regression perform if errors are heteroskedastic? (2) how does the presence of missing data affect treatment effect estimates from a propensity score analysis? (3) how does cluster robust variance estimation perform when the number of clusters is small? To answer such questions, we conduct experiments by simulating thousands of datasets based on pseudo-random sampling, applying statistical methods, and evaluating how well those statistical methods recover the true data-generating conditions (Morris et al., 2019).\nThe goal of simhelpers is to assist in running simulation studies. The main tools in the package consist of functions to calculate measures of estimator performance like bias, root mean squared error, rejection rates. The functions also calculate the associated Monte Carlo standard errors (MCSE) of the performance measures. These functions are divided into three major categories of performance criteria: absolute criteria, relative criteria, and criteria to evaluate hypothesis testing. The functions use the tidyeval principles, so that they play well with dplyr and fit easily into a %&gt;%-centric workflow (Wickham et al., 2019).In addition to the set of functions that calculates performance measures and MCSE, the package also includes a function, create_skeleton(), that generates a skeleton outline for a simulation study. Another function, evaluate_by_row(), runs the simulation for each combination of conditions row by row. This function uses future_pmap() from the furrr package, making it easy to run the simulation in parallel (Vaughan & Dancho, 2018). The package also includes several datasets that contain results from example simulation studies."
  },
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Megha Joshi",
    "section": "",
    "text": "Download\n  \n\n\n  \n\n\n\n\n Back to top"
  },
  {
    "objectID": "publications/missing_data/index.html",
    "href": "publications/missing_data/index.html",
    "title": "The performance of multivariate methods for two-group comparisons with small samples and incomplete data",
    "section": "",
    "text": "In intervention studies having multiple outcomes, researchers often use a series of univari-ate tests (e.g., ANOVAs) to assess group mean differences. Previous research found that thisapproach properly controls Type I error and generally provides greater power compared toMANOVA, especially under realistic effect size and correlation combinations. However, whengroup differences are assessed for a specific outcome, these procedures are strictly univari-ate and do not consider the outcome correlations, which may be problematic with missingoutcome data. Linear mixed or multivariate multilevel models (MVMMs), implemented withmaximum likelihood estimation, present an alternative analysis option where outcome cor-relations are taken into account when specific group mean differences are estimated. In thisstudy, we use simulation methods to compare the performance of separate independentsamplesttests estimated with ordinary least squares and analogousttests from MVMMs toassess two-group mean differences with multiple outcomes under small sample and miss-ingness conditions. Study results indicated that a MVMM implemented with restricted maximum likelihood estimation combined with the Kenward–Roger correction had the best performance. Therefore, for intervention studies with smallNand normally distributed multi-variate outcomes, the Kenward–Roger procedure is recommended over traditional methods and conventional MVMM analyses, particularly with incomplete data.\n\n\n\n Back to topCitationBibTeX citation:@article{a._pituch2019,\n  author = {A. Pituch, Keenan and Joshi, Megha and E Cain, Molly and A\n    Whittaker, Tiffany and Chang, Wanchen and Park, Ryoungsun and J\n    McDougall, Graham},\n  title = {The Performance of Multivariate Methods for Two-Group\n    Comparisons with Small Samples and Incomplete Data},\n  journal = {Multivariate Behavioral Research},\n  volume = {55},\n  number = {5},\n  pages = {704-721},\n  date = {2019-09-25},\n  url = {https://doi.org/10.1080/00273171.2019.1667217},\n  doi = {10.1080/00273171.2019.1667217},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. Pituch, K., Joshi, M., E Cain, M., A Whittaker, T., Chang, W., Park,\nR., & J McDougall, G. (2019). The performance of multivariate\nmethods for two-group comparisons with small samples and incomplete\ndata. Multivariate Behavioral Research, 55(5),\n704–721. https://doi.org/10.1080/00273171.2019.1667217"
  },
  {
    "objectID": "posts/ipd_meta/index.html",
    "href": "posts/ipd_meta/index.html",
    "title": "Individual Participant Data Meta-Analysis: Example with R",
    "section": "",
    "text": "Traditional meta-analyses use aggregated or summary level information from studies or reports (Cooper & Patall, 2009; Riley, Lambert, & Abo-Zaid, 2010). Analysts conducting aggregated data meta-analysis would look up relevant literature and code summary statistics needed to calculate one or more effect sizes from each study and also code the corresponding moderator variables. And, then run meta-regression models to (1) summarize effect size estimates across studies, (2) characterize variability in effect sizes across studies, and (3) explain the variability in the effect sizes. Moderator variables will be at the effect size-level or the study-level (e.g., the outcome measure or the percentage of economically disadvantaged students in the sample used to calculate the effect). One drawback of aggregated data meta-analysis is that we cannot examine the effect of moderators at the individual level. For example, we can say that studies with higher percentage of economically disadvantaged students tend to have higher effects of some treatment. However, we cannot say that the treatment works better for economically disadvantaged students. To do so would be to commit ecological fallacy (Geissbühler, Hincapié, Aghlmandi, et al., 2021).\nAnother way of conducting meta-analysis is to use individual participant-level data instead of aggregated summaries (Riley et al., 2010). For each study in the meta-analysis, the analyst would have access to the individual-level data. Outcomes and moderator variables will be at the individual level (e.g., students’ scores on an achievement test and indicator for whether or not they are economically disadvantaged). Because data is at the individual-level, analysts can conduct subgroup analyses to examine the potentially heterogeneous effects of a treatment for different subgroups (Cooper & Patall, 2009). The feasibility of conducting such subgroup analyses provides IPDMA a major advantage over aggregated data meta-analysis, which heavily rely on the analyses conducted by primary study authors who may not have reported results from subgroup analyses."
  },
  {
    "objectID": "posts/ipd_meta/index.html#aggregated-data-meta-analysis-and-ipdma",
    "href": "posts/ipd_meta/index.html#aggregated-data-meta-analysis-and-ipdma",
    "title": "Individual Participant Data Meta-Analysis: Example with R",
    "section": "",
    "text": "Traditional meta-analyses use aggregated or summary level information from studies or reports (Cooper & Patall, 2009; Riley, Lambert, & Abo-Zaid, 2010). Analysts conducting aggregated data meta-analysis would look up relevant literature and code summary statistics needed to calculate one or more effect sizes from each study and also code the corresponding moderator variables. And, then run meta-regression models to (1) summarize effect size estimates across studies, (2) characterize variability in effect sizes across studies, and (3) explain the variability in the effect sizes. Moderator variables will be at the effect size-level or the study-level (e.g., the outcome measure or the percentage of economically disadvantaged students in the sample used to calculate the effect). One drawback of aggregated data meta-analysis is that we cannot examine the effect of moderators at the individual level. For example, we can say that studies with higher percentage of economically disadvantaged students tend to have higher effects of some treatment. However, we cannot say that the treatment works better for economically disadvantaged students. To do so would be to commit ecological fallacy (Geissbühler, Hincapié, Aghlmandi, et al., 2021).\nAnother way of conducting meta-analysis is to use individual participant-level data instead of aggregated summaries (Riley et al., 2010). For each study in the meta-analysis, the analyst would have access to the individual-level data. Outcomes and moderator variables will be at the individual level (e.g., students’ scores on an achievement test and indicator for whether or not they are economically disadvantaged). Because data is at the individual-level, analysts can conduct subgroup analyses to examine the potentially heterogeneous effects of a treatment for different subgroups (Cooper & Patall, 2009). The feasibility of conducting such subgroup analyses provides IPDMA a major advantage over aggregated data meta-analysis, which heavily rely on the analyses conducted by primary study authors who may not have reported results from subgroup analyses."
  },
  {
    "objectID": "posts/ipd_meta/index.html#ipdma-analyses-in-r",
    "href": "posts/ipd_meta/index.html#ipdma-analyses-in-r",
    "title": "Individual Participant Data Meta-Analysis: Example with R",
    "section": "IPDMA Analyses in R",
    "text": "IPDMA Analyses in R\nThere are two ways do conduct IPDMA: (1) one-stage meta-analysis which involves analyzing data from all studies at once; and, (2) two-stage meta-analysis which involves first analyzing individual data separately for each primary study and then synthesizing the effects using meta-regression models (Cooper & Patall, 2009; Riley et al., 2010). I will walk through how to run each of these using an example data.\n\nExample Dataset\nI’ve tried to find a publicly available IPDMA dataset to use as an example. However, I haven’t found one appropriate for this post. Thus, I am using a dataset is from a block randomized study (Bryan et al., 2016). Blocks can be somewhat thought of as different studies in a meta-analysis (not the same thing but please go along for this post). In the data, students were randomized within classes. There are 30 classrooms (like 30 different studies) within which participants were randomized. The study examined the effects of brief psychological interventions on eating behaviors. The outcome that I am going to analyze is autonprosocial, four-item self-report measure of alignment of healthy eating with adolescent values.\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(estimatr)\nlibrary(metafor)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(kableExtra)\nlibrary(janitor)\n\noptions(knitr.kable.NA = '')\n\nbryan_dat &lt;- read_csv(\"https://raw.githubusercontent.com/meghapsimatrix/datasets/master/causal/bryan_dat.csv\")\n\nglimpse(bryan_dat %&gt;% select(classroom, condition, condition_collapsed, female, autonprosocial))\n\nRows: 501\nColumns: 5\n$ classroom           &lt;chr&gt; \"B2\", \"B2\", \"A1\", \"A2\", \"B3\", \"A3\", \"B4\", \"B2\", \"D…\n$ condition           &lt;chr&gt; \"expose treatment\", \"expose treatment\", \"no treatm…\n$ condition_collapsed &lt;chr&gt; \"expose treatment\", \"expose treatment\", \"control\",…\n$ female              &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,…\n$ autonprosocial      &lt;dbl&gt; 3.75, 4.25, 2.25, 4.25, 3.25, 3.25, 4.25, 1.25, 2.…\n\n\n\n\nOne Stage IPDMA\nOne stage IPDMA basically involves running one analysis on all the data accounting for clustering by study. Below I am running HLM using lmer() from lme4 package specifying fixed intercepts but random slopes for treatment effect by classroom. The model that I am using is based on suggestion by Bloom, Raudenbush, Weiss, & Porter (2017). The results table shows the treatment effect estimate (and it’s se etc.) and also the estimate of the standard deviation of the treatment effects across classrooms.\n\n# creating treatment indicator\nbryan_dat &lt;-\n  bryan_dat %&gt;%\n  mutate(trt_ind = as.integer(condition_collapsed == \"expose treatment\"))\n\n\n# function to estimate treatment effects\nestimate_effects &lt;- function(dat, stage){\n  \n  if(stage == \"one\"){\n    \n    mod &lt;- lmer(autonprosocial ~ 0 + classroom + trt_ind + \n                (0 + trt_ind | classroom), \n                data = dat)\n  \n  } else if(stage == \"two\") {\n    \n    mod &lt;- lm_robust(autonprosocial ~ trt_ind, data = dat)\n    \n  }\n  \n  res &lt;- tidy(mod) %&gt;%\n    filter(str_detect(term, \"trt_ind\")) %&gt;%\n    clean_names() %&gt;%\n    mutate(v = std_error^2)\n  \n  return(res)\n  \n}\n\n\n\n# estimate of treatment effect and the sd of treatment effect across classrooms\nestimate_effects(bryan_dat, stage = \"one\") %&gt;%\n  kable(digits = 3) \n\n\n\n\neffect\ngroup\nterm\nestimate\nstd_error\nstatistic\ndf\np_value\nv\n\n\n\n\nfixed\n\ntrt_ind\n1.157\n0.083\n14.014\n25.776\n0\n0.007\n\n\nran_pars\nclassroom\nsd__trt_ind\n0.146\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubgroup Analyses\nBelow I am creating subgroups based on the female variable and estimating the treatment effect and the sd of treatment effects across classrooms for each subgroup:\n\nbryan_dat &lt;- bryan_dat %&gt;%\n  mutate(female = ifelse(female == 1, \"Female\", \"Not Female\"))\n\nbryan_dat %&gt;%\n  group_by(female) %&gt;%\n  do(estimate_effects(., stage = \"one\")) %&gt;%\n  kable(digits = 3)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\nfemale\neffect\ngroup\nterm\nestimate\nstd_error\nstatistic\ndf\np_value\nv\n\n\n\n\nFemale\nfixed\n\ntrt_ind\n1.217\n0.109\n11.187\n232\n0\n0.012\n\n\nFemale\nran_pars\nclassroom\nsd__trt_ind\n0.000\n\n\n\n\n\n\n\nNot Female\nfixed\n\ntrt_ind\n1.043\n0.115\n9.058\n207\n0\n0.013\n\n\nNot Female\nran_pars\nclassroom\nsd__trt_ind\n0.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Stage IPDMA\n\nFirst Stage: Primary Study Analysis\nFirst, I estimate the average treatment effect by block. For IPDMA, we would estimate the treatment effects for each primary study:\n\nfirst_stage_res &lt;-\n  bryan_dat %&gt;%\n  group_by(classroom) %&gt;%\n  do(estimate_effects(., stage = \"two\")) \n\nglimpse(first_stage_res)\n\nRows: 30\nColumns: 11\nGroups: classroom [30]\n$ classroom &lt;chr&gt; \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"B1\", \"B2\", \"B3\", \"B4\", …\n$ term      &lt;chr&gt; \"trt_ind\", \"trt_ind\", \"trt_ind\", \"trt_ind\", \"trt_ind\", \"trt_…\n$ estimate  &lt;dbl&gt; 1.7951128, 1.4062500, 0.1250000, 0.9318182, 0.9130952, 1.333…\n$ std_error &lt;dbl&gt; 0.2744876, 0.2830958, 0.3683758, 0.2781655, 0.3595215, 0.496…\n$ statistic &lt;dbl&gt; 6.5398688, 4.9673998, 0.3393274, 3.3498700, 2.5397511, 2.687…\n$ p_value   &lt;dbl&gt; 9.175979e-07, 3.266890e-04, 7.387762e-01, 4.386511e-03, 2.11…\n$ conf_low  &lt;dbl&gt; 1.2285983, 0.7894372, -0.6559219, 0.3389225, 0.1545711, 0.28…\n$ conf_high &lt;dbl&gt; 2.3616273, 2.0230628, 0.9059219, 1.5247139, 1.6716194, 2.380…\n$ df        &lt;dbl&gt; 24, 12, 16, 15, 17, 17, 7, 32, 26, 19, 16, 10, 12, 11, 18, 1…\n$ outcome   &lt;chr&gt; \"autonprosocial\", \"autonprosocial\", \"autonprosocial\", \"auton…\n$ v         &lt;dbl&gt; 0.07534343, 0.08014323, 0.13570076, 0.07737603, 0.12925574, …\n\n\n\n\nSecond Stage: Meta-Analysis\nThen, in the second stage, I synthesize the block specific treatment effects using metafor::rma.uni(), which weighs each effect size estimate by its precision:\n\nsecond_stage_res &lt;- rma.uni(yi = estimate, \n                            sei = std_error,\n                            data = first_stage_res, \n                            method = \"REML\", \n                            test = \"knha\")\n\ntibble(\n  rowname = rownames(second_stage_res$b),\n  estimate = as.vector(second_stage_res$b),\n  SE = second_stage_res$se,\n  ci_lo = second_stage_res$ci.lb,\n  ci_hi = second_stage_res$ci.ub,\n  tau_2 = second_stage_res$tau2\n) %&gt;%\n  kable(digits = 3)\n\n\n\n\nrowname\nestimate\nSE\nci_lo\nci_hi\ntau_2\n\n\n\n\nintrcpt\n1.214\n0.088\n1.034\n1.394\n0.13\n\n\n\n\n\n\n\n\n\nSubgroup Analyses\n\n\nFirst Stage: Primary Study Analysis\nHere, I estimate treatment effect by classroom and by the female variable:\n\nsubgroup_fs_res &lt;- bryan_dat %&gt;%\n  group_by(classroom, female) %&gt;%\n  do(estimate_effects(., stage = \"two\"))\n\n1 coefficient  not defined because the design matrix is rank deficient\n\n\n\n\nSecond Stage: Meta-Analysis\nThen, in the second stage, I synthesize the block specific subgroup effects.\n\nestimate_subgroup_mod &lt;- function(dat){\n  \n  second_stage_res &lt;- rma.uni(yi = estimate, \n                              sei = std_error,\n                              data = dat, \n                              method = \"REML\", \n                              test = \"knha\")\n  \n  res &lt;- tibble(\n      rowname = rownames(second_stage_res$b),\n      est = as.vector(second_stage_res$b),\n      SE = second_stage_res$se,\n      ci_lo = second_stage_res$ci.lb,\n      ci_hi = second_stage_res$ci.ub,\n      tau_2 = second_stage_res$tau2\n    )\n\n  return(res)\n  \n}\n  \nsubgroup_fs_res %&gt;%\n  group_by(female) %&gt;%\n  do(estimate_subgroup_mod(.)) %&gt;%\n  kable(digits = 3)\n\nWarning: 2 studies with NAs omitted from model fitting.\n\n\nWarning: 1 study with NAs omitted from model fitting.\n\n\n\n\n\nfemale\nrowname\nest\nSE\nci_lo\nci_hi\ntau_2\n\n\n\n\nFemale\nintrcpt\n1.234\n0.124\n0.979\n1.489\n0.290\n\n\nNot Female\nintrcpt\n1.179\n0.101\n0.972\n1.387\n0.171"
  },
  {
    "objectID": "posts/I_squared/index.html",
    "href": "posts/I_squared/index.html",
    "title": "What Should I Use to Quantify Heterogeneity in Meta-Analysis?",
    "section": "",
    "text": "Scientific researchers tend to produce literature on the same topic either to replicate or extend prior studies or due to a lack of awareness of prior evidence (Hedges & Cooper, 2009). Results across studies tend to vary, even when researchers try to replicate studies, due to differences in sample characteristics, research designs, analytic strategies or sampling error (Hedges & Cooper, 2009). Meta-analysis is a set of statistical techniques for synthesizing results from multiple primary studies on a common topic. The three major goals of meta-analysis include: (1) summarizing effect size estimates across studies, (2) characterizing, and (3) explaining the variability in the effect sizes.\nIn this post I am going to talk about the second goal: characterizing variability in effect sizes.\n\n\nPooling effect size estimates provides an average estimate of the effect of an intervention. However, typically, researchers are also interested in the existence of variation in the effects (Konstantopoulos & Hedges, 2019). The characterization of variation in effects across studies in a meta-analysis allows meta-analysts to gauge how much the effects vary across different studies included in the meta-analysis and whether it makes sense to pool the effects together. Are we comparing apples to apples or comparing apples to oranges and pears :D\n\n\nI will use the following notations in this post:\n\n\\(\\hat{\\sigma}^2\\) refers to the estimated sampling variance of primary study level effects. This variance corresponds to the sampling error - in primary studies we are estimating the effects from a sample so we have sampling error.\n\\(\\hat{\\tau}^2\\) refers to the variation in effects across studies included in the meta-analysis - the between-study variation in effects.\n\n\n\n\nA commonly used statistic to measure heterogeneity is \\(I^2\\) (Higgins & Thompson, 2002). It is a descriptive statistic that denotes the percentage or proportion of variance in the observed effect size estimates that is due to variation in the true effect sizes (Borenstein, Higgins, Hedges, & Rothstein, 2017).\n\\[ I^2 = \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\hat{\\sigma} ^2}\\]\nThe formula for \\(I^2\\) has the between-study variance estimate in the numerator and the total variance in the denominator - which is the sum of between-study variance and the sampling variance. \\(I^2\\) is a relative measure of heterogeneity in that it measures the proportion of total heterogeneity that is between studies.\nBorenstein et al. (2017) noted that the value of \\(I^2\\) often is misinterpreted in applied meta-analysis. The value of \\(I^2\\) as we can see in the formula above depends not only on between-study variation in effects (measured by \\(\\hat\\tau^2\\)) but also on the sampling variance (measured by \\(\\hat\\sigma^2\\)).\n\\(I^2\\) values tend to be high when the value of \\(\\hat\\sigma^2\\) is small which can happen if a meta-analysis includes many primary studies with large sample sizes. Primary studies with large sample sizes tend to produce less noisy estimates, hence the \\(\\hat\\sigma^2\\) value will be small for these kinds of studies. \\(I^2\\) values can be small when the value of \\(\\hat\\sigma^2\\) is large which can happen if meta-analysis includes many primary studies with small sample sizes. Note that in both of the cases, the absolute between-study variance (\\(\\hat\\tau^2\\)) is not even considered.\nBelow is an example code where I am holding the between-study variation in effect sizes constant at .5. By just changing the magnitude of the sampling variance (\\(\\hat\\sigma^2\\)), the \\(I^2\\) value decreases by a lot, from 98% to 62.5%. Therefore Borenstein et al. (2017) argued that the value of \\(I^2\\) is not very useful in quantifying the amount of actual between-study heterogeneity present in meta-analyses.\n\ntau_sq &lt;- .5\nsigma_sq &lt;- .01  \n\nI_sq &lt;- tau_sq / (tau_sq + sigma_sq)\nI_sq\n\n[1] 0.9803922\n\n\n\n# i am not changing tau_sq\nsigma_sq &lt;- .3\n\nI_sq &lt;- tau_sq / (tau_sq + sigma_sq)\nI_sq\n\n[1] 0.625\n\n\n\n\n\n\\(I^2\\) is a relative measure of heterogeneity and depends not only on between-study variation in effects but also on sampling error. An absolute measure of between-study variation in effects is a descriptive statistic called \\(\\tau^2\\). It is the variance in effects across primary studies included in a meta-analysis. Viechtbauer (2007) described \\(\\tau^2\\) as the estimated variance of the random variable producing the true effect sizes. The square root form, \\(\\tau\\), is the standard deviation of effects across studies. \\(\\tau\\) will be in the same scale as the effect size estimates. \\(\\tau\\) can be interpreted just like the regular standard deviation. In the context of the scale of the effect sizes, it quantifies how much variation there is across the effects from different studies."
  },
  {
    "objectID": "posts/I_squared/index.html#what-is-heterogeneity",
    "href": "posts/I_squared/index.html#what-is-heterogeneity",
    "title": "What Should I Use to Quantify Heterogeneity in Meta-Analysis?",
    "section": "",
    "text": "Pooling effect size estimates provides an average estimate of the effect of an intervention. However, typically, researchers are also interested in the existence of variation in the effects (Konstantopoulos & Hedges, 2019). The characterization of variation in effects across studies in a meta-analysis allows meta-analysts to gauge how much the effects vary across different studies included in the meta-analysis and whether it makes sense to pool the effects together. Are we comparing apples to apples or comparing apples to oranges and pears :D\n\n\nI will use the following notations in this post:\n\n\\(\\hat{\\sigma}^2\\) refers to the estimated sampling variance of primary study level effects. This variance corresponds to the sampling error - in primary studies we are estimating the effects from a sample so we have sampling error.\n\\(\\hat{\\tau}^2\\) refers to the variation in effects across studies included in the meta-analysis - the between-study variation in effects.\n\n\n\n\nA commonly used statistic to measure heterogeneity is \\(I^2\\) (Higgins & Thompson, 2002). It is a descriptive statistic that denotes the percentage or proportion of variance in the observed effect size estimates that is due to variation in the true effect sizes (Borenstein, Higgins, Hedges, & Rothstein, 2017).\n\\[ I^2 = \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\hat{\\sigma} ^2}\\]\nThe formula for \\(I^2\\) has the between-study variance estimate in the numerator and the total variance in the denominator - which is the sum of between-study variance and the sampling variance. \\(I^2\\) is a relative measure of heterogeneity in that it measures the proportion of total heterogeneity that is between studies.\nBorenstein et al. (2017) noted that the value of \\(I^2\\) often is misinterpreted in applied meta-analysis. The value of \\(I^2\\) as we can see in the formula above depends not only on between-study variation in effects (measured by \\(\\hat\\tau^2\\)) but also on the sampling variance (measured by \\(\\hat\\sigma^2\\)).\n\\(I^2\\) values tend to be high when the value of \\(\\hat\\sigma^2\\) is small which can happen if a meta-analysis includes many primary studies with large sample sizes. Primary studies with large sample sizes tend to produce less noisy estimates, hence the \\(\\hat\\sigma^2\\) value will be small for these kinds of studies. \\(I^2\\) values can be small when the value of \\(\\hat\\sigma^2\\) is large which can happen if meta-analysis includes many primary studies with small sample sizes. Note that in both of the cases, the absolute between-study variance (\\(\\hat\\tau^2\\)) is not even considered.\nBelow is an example code where I am holding the between-study variation in effect sizes constant at .5. By just changing the magnitude of the sampling variance (\\(\\hat\\sigma^2\\)), the \\(I^2\\) value decreases by a lot, from 98% to 62.5%. Therefore Borenstein et al. (2017) argued that the value of \\(I^2\\) is not very useful in quantifying the amount of actual between-study heterogeneity present in meta-analyses.\n\ntau_sq &lt;- .5\nsigma_sq &lt;- .01  \n\nI_sq &lt;- tau_sq / (tau_sq + sigma_sq)\nI_sq\n\n[1] 0.9803922\n\n\n\n# i am not changing tau_sq\nsigma_sq &lt;- .3\n\nI_sq &lt;- tau_sq / (tau_sq + sigma_sq)\nI_sq\n\n[1] 0.625\n\n\n\n\n\n\\(I^2\\) is a relative measure of heterogeneity and depends not only on between-study variation in effects but also on sampling error. An absolute measure of between-study variation in effects is a descriptive statistic called \\(\\tau^2\\). It is the variance in effects across primary studies included in a meta-analysis. Viechtbauer (2007) described \\(\\tau^2\\) as the estimated variance of the random variable producing the true effect sizes. The square root form, \\(\\tau\\), is the standard deviation of effects across studies. \\(\\tau\\) will be in the same scale as the effect size estimates. \\(\\tau\\) can be interpreted just like the regular standard deviation. In the context of the scale of the effect sizes, it quantifies how much variation there is across the effects from different studies."
  },
  {
    "objectID": "posts/missing_dat/index.html",
    "href": "posts/missing_dat/index.html",
    "title": "Missing Data in Propensity Score Analysis",
    "section": "",
    "text": "Theories behind propensity score analysis assume that the covariates are fully observed (Rosenbaum & Rubin, 1983, 1984). However, in practice, observational analyses require large administrative databases or surveys, which inevitably will have missingness in the covariates. The response patterns of people with missing covariates may be different than those of people with observed data (Mohan, Pearl, & Tian, 2013). Therefore, ways to handle missing covariate data need to be examined. The basic estimation of propensity scores using logistic regression will delete cases with missing data, which can be problematic as it can cause bias in the treatment effect estimates (Baraldi & Enders, 2010)."
  },
  {
    "objectID": "posts/missing_dat/index.html#missing-data-methods-in-propensity-score-analysis",
    "href": "posts/missing_dat/index.html#missing-data-methods-in-propensity-score-analysis",
    "title": "Missing Data in Propensity Score Analysis",
    "section": "Missing Data Methods in Propensity Score Analysis",
    "text": "Missing Data Methods in Propensity Score Analysis\nBelow I explain three major methods used in the applied propensity score analysis literature when \\(X\\) is not fully observed. I also explain three other methods to handle missing data that are not commonly used in applied literature but have been proposed theoretically. I also describe the assumptions about missing data and strong ignorability underlying each of the methods. Let \\(X_{obs}\\) indicate the observed parts of \\(X\\) and \\(X_{mis}\\) indicate the missing parts of \\(X\\). \\(D\\) indicates the fully observed treatment indicator and \\(Y\\) indicates a fully observed outcome variable.\n\nComplete Case Analysis\nThis approach deletes cases with missing data in any of the variables used in the analysis (Baraldi & Enders, 2010; Hill, 2004). The traditional propensity score estimation method of using logistic regression implements complete case analysis by default. Therefore, this method is commonly used in applied research. The data that remains after deleting cases with missing data are assumed to be a simple random sample of the full data (Baraldi & Enders, 2010). Missingness is not related to any study variables nor to the hypothetically complete values of itself. According to Hill (2004), the assumption underlying complete case analysis is that the joint distributions of \\(X_{obs}\\) and \\(X_{mis}\\) are same across the two treatment conditions: \\[\\begin{equation}\nX_{obs}, X_{mis} \\perp\\!\\!\\!\\perp D\n\\end{equation}\\] Therefore, an unbiased causal effect estimate can be retrieved after deleting cases with missing data. Such an assumption is very stringent and unlikely to be met in the types of data required for propensity score analyses (Baraldi & Enders, 2010; Hill, 2004). As mentioned above, deleting cases can also result in loss of power. Additionally, whether \\(X_{mis}\\) is balanced between the treatment groups cannot be confirmed.\n\n\nMultiple Imputation\nMultiple imputation (MI) generates multiple sets of data with the missing values drawn from an imputation model (Mitra & Reiter, 2016; Rubin, 1987). MI will create \\(m &gt; 1\\) imputed datasets that contain different imputed values (Murray, 2018; van Buuren, 2018). Analyses can be performed on each of the datasets and results from each dataset can be aggregated across to derive a final estimate, standard error, degrees of freedom, and test result. Thus, MI involves two stages: (1) imputation and creation of the \\(m\\) imputed datasets, and (2) analysis and pooling of estimates across the datasets (Murray, 2018; van Buuren, 2018).\nThere are two approaches for imputing multivariate missing data: (1) joint modeling, JM, and (2) fully conditional specification, FCS, also called multivariate imputation by chained equations, MICE (Murray, 2018; van Buuren, 2018; van Buuren & Groothuis-Oudshoorn, 2011). JM entails jointly modeling variables with missingness by drawing from a multivariate distribution (Murray, 2018; van Buuren, 2018; van Buuren & Groothuis-Oudshoorn, 2011). FCS entails univariate conditional imputation models of variables with missing data that iteratively condition on all other variables using Monte Carlo Markov chain methods (van Buuren, 2018; van Buuren & Groothuis-Oudshoorn, 2011). JM imputes all variables simultaneously whereas FCS imputes one variable at a time (van Buuren, 2018). Because JM requires specification of a joint distribution for all the variables, it may not be as flexible as FCS when dealing with a large number of covariates with missing data (Akande, Li, & Reiter, 2017). However, FCS is computationally more intensive than JM (van Buuren, 2018). FCS also has been shown to outperform JM for categorical variables and is more robust under mis-specification of imputation model (van Buuren, 2018). Therefore, van Buuren (2018) recommended to use FCS over JM.\nIf the missingness mechanism is MAR or MCAR and if assumptions underlying the imputation model are correct, MI will yield unbiased results, as it uses the information available in \\(X_{obs}\\) to impute missing values (Murray, 2018). In the causal inference context, Hill (2004) argued that MI relies on the assumption of latent ignorability, a concept introduced by Frangakis & Rubin (1999). The assumption requires that the treatment assignment mechanism is ignorable given complete covariate data including the values that are latent or missing. These missing values are derived from MI. Below, let \\(e_{MI}(X)\\) denote propensity scores derived after multiple imputation: \\[\\begin{equation}\nX_{obs}, X_{mis} \\perp\\!\\!\\!\\perp D| e_{MI}(X)\n\\end{equation}\\] \\[\\begin{equation}\nY(1), Y(0) \\perp\\!\\!\\!\\perp D | e_{MI}(X)\n\\end{equation}\\] Hill (2004) proposed two different ways to combine propensity scores estimated in each of the m datasets:\n\nMultiple Imputation Across (MI Across)\nThis approach involves creating m imputed datasets and then estimating propensity scores within each of the datasets and then averaging each unit’s m propensity scores across the m datasets (Hill, 2004). Stratification, matching or IPW can be implemented using these averaged propensity scores (Hill, 2004). Outcome models that include covariates will need to use the weights or strata derived from the averaged propensity scores and the m sets of covariate values. The weighted regression estimates will then need to be pooled.\n\n\nMultiple Imputation Within (MI Within)\nThis approach involves creating m imputed datasets and then estimating propensity scores within each of the datasets (Hill, 2004). Instead of averaging the propensity scores across the datasets, this method entails conditioning on the propensity scores within the datasets and running the outcome analyses within each dataset (Hill, 2004). The separate regression estimates have to be pooled.\n\n\n\nGeneralized Propensity Scores\nRosenbaum & Rubin (1984) proposed the use of generalized propensity scores (GPS) as a way to address missing covariate data. The GPS represents the probability of treatment given observed covariates and missingness indicators (Rosenbaum & Rubin, 1984): \\[\\begin{equation}\ne^*(X) = P(D = 1|X_{obs}, R)\n\\end{equation}\\] Conditioning on \\(e^*(X)\\) will balance the treatment groups in terms of the observed covariates and missingness patterns (Rosenbaum & Rubin, 1984). The observed part of \\(X\\) and the missingness pattern indicators, \\(R\\), will be independent of treatment assignment given the GPS (Rosenbaum & Rubin, 1984): \\[\\begin{equation}\nX_{obs}, R \\perp\\!\\!\\!\\perp D| e^*(X)\n\\end{equation}\\] However, conditioning on GPS will not balance the groups in terms of the unobserved values of \\(X\\) (Rosenbaum & Rubin, 1984): \\[\\begin{equation}\nX_{mis} \\not\\!\\perp\\!\\!\\!\\perp D| e^*(X)\n\\end{equation}\\] Although this technique of treating missing data is not generally recommended for other types of missing data analyses, it has been recommended for use in propensity score analysis literature (Rosenbaum & Rubin, 1984; Stuart, 2010). In the context of propensity score analysis, this approach does not assume latent ignorability of treatment assignment because legitimate values for missing data are never derived. The assumption underlying this method is that balancing the treatment and control groups on \\(X_{obs}\\) and \\(R\\) is a sufficient condition to satisfy ignorability. With the GPS, the treatment and control groups are possibly not going to be balanced in terms of \\(X_{mis}\\).\nFor large studies with few missing data patterns, Rosenbaum & Rubin (1984) suggested estimating separate logit models for each missingness pattern. In practice, it is common to encounter many patterns of missing data. For these scenarios, Rosenbaum & Rubin (1984) suggested creating an additional category indicating missingness for categorical variables. For continuous variables, Stuart (2010) recommended imputing missing data with a single arbitrary value, such as the overall mean of the covariate, and then creating a missingness indicator variable. In general missing data analysis context, van Buuren (2018) noted that this method of combining arbitrary (mean) imputation along with missingness indicators can underestimate the standard error of the estimate of interest.\nThe CART algorithms treat missing data natively as they split missingness as a category itself. In this manner, this approach is similar to the GPS which uses missingness pattern indicators when estimating propensity scores. The missingness categories are used to estimate propensity scores and conditioning on the propensity scores should balance the treatment and control condition in terms of the patterns. However, splitting does not actually impute the missing data so it is plausible to assume that like GPS, scores derived using the splitting method will not balance the groups in terms of the latent missing data. In addition, unlike MI, there are no imputed complete datasets saved to analyze for the outcome model. Therefore, splitting would need to be combined with some other technique for outcome modeling.\n\n\nOther Methods\nThe following methods have been discussed theoretically in literature examining missing data methods in propensity score analysis. However, these methods are not commonly used in applied literature.\n\nComplete Variables\nThis method removes any variable with missing data (Hill, 2004). By removing variables with missing data, the approach assumes that the distribution of those variables (both the observed and missing parts) are the same across the two treatment groups (Hill, 2004). If this assumption does not hold, then this method can result in bias in treatment effect estimates due to removal of important confounding variables (Hill, 2004).\n\n\nD’Agostino and Rubin Expectation Maximization\nAnother approach is a method introduced by D’Agostino & Rubin (2000), which estimates propensity scores using an Expectation Conditional Maximization (ECM) algorithm (Hill, 2004). This method, DR, works similar to GPS as it models \\(X_{obs}\\), \\(R\\), and the treatment indicator variable. However, instead of imputing \\(X_{mis}\\), the DR method uses ECM to estimate propensity scores in presence of missing data (Hill, 2004). The assumption underlying DR is that within each missingness pattern defined by \\(R\\), \\(X_{mis}\\) is independent of \\(D\\) given the observed data, \\(X_{obs}\\) (Hill, 2004): \\[\\begin{equation}\nX_{mis} \\perp\\!\\!\\!\\perp D| X_{obs}, R\n\\end{equation}\\] Such independence is sufficient to satisfy the ignorability assumption in presence of missing covariate data. With this method, the assumption cannot be checked, however, as DR does not actually impute the missing values. This method is not readily available in commonly used software like R.\n\n\nMultiple Imputation Missingness Indicator Pattern Mixutre\nQu & Lipkovich (2009) extended MI by introducing the missingness indicator pattern mixture (MIMP) approach, which is the same as MI but adds \\(R\\) in the propensity score estimation model. The rationale behind this approach is to use information given by missingness patterns to estimate treatment propensities. The method will assume latent ignorabilty. However, this approach should also balance the treatment group on \\(R\\) as \\(R\\) is used to estimate the propensity scores: \\[\\begin{equation}\nX_{obs}, X_{mis}, R \\perp\\!\\!\\!\\perp D| e_{MIMP}(X)\n\\end{equation}\\] Qu & Lipkovich (2009) argued that extending MI by adding R to the propensity score estimation accounts for non-ignorability or MNAR (Qu & Lipkovich, 2009; van Buuren, 2018). This method allows missingness itself to provide information on missingness: \\[\\begin{equation}\nP(X| X_{obs}, R = 1) \\neq P(X| X_{obs}, R = 0)\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/CI/index.html",
    "href": "posts/CI/index.html",
    "title": "What is a Confidence Interval?",
    "section": "",
    "text": "library(tidyverse)\nlibrary(knitr)"
  },
  {
    "objectID": "posts/CI/index.html#population-distribution",
    "href": "posts/CI/index.html#population-distribution",
    "title": "What is a Confidence Interval?",
    "section": "Population Distribution",
    "text": "Population Distribution\nThere are around 15 million adults in Texas and I am interested in estimating the average height of all adult Texans. I cannot go out and measure everyone’s height due to financial, capacity constraints etc.\nFor the sake of this example, I am going to draw up a hypothetical population distribution of heights of all people in Texas. Below is a histogram showing the distribution of the heights across all adults in the population:\n\noptions(scipen = 10000)\nset.seed(04302021)\n\n# i am setting min 4 and max 7\nheights &lt;- tibble(heights = pmax(4, pmin(7, rnorm(n = 15000000, mean = 5.5, sd = .5))))\n\nggplot(heights, aes(x = heights)) +\n  geom_histogram(alpha = 0.5, \n                 bins = 50, \n                 fill = \"blue4\") +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"Heights of All Texas Adults\", \n       y = \"Number of People in Texas\",\n       title = \"Population Distribution of Heights in Texas\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe population distribution is the distribution of heights of all adults in Texas.\nThe mean of the distribution is 5.5 and the standard deviation of the distribution is 0.499. These are the population parameters:\n\nmean(heights$heights)\n\nsd(heights$heights)\n\nThe heights range from 4 to 7 ~ this is the range of true heights in the population, which we won’t typically know because we don’t go out and measure heights of 15 million people."
  },
  {
    "objectID": "posts/CI/index.html#sample-distribution",
    "href": "posts/CI/index.html#sample-distribution",
    "title": "What is a Confidence Interval?",
    "section": "Sample Distribution",
    "text": "Sample Distribution\nI can’t go out and measure everyone in the population so I take a sample of 10. Here is the distribution of heights in that sample:\n\nset.seed(05012021)\nsample_1 &lt;- tibble(heights = sample(heights$heights, size = 10))\n\nggplot(sample_1, aes(x = heights)) +\n  geom_histogram(alpha = 0.5, \n                 bins = 10, \n                 fill = \"darkolivegreen\") +\n  scale_y_continuous(breaks = seq(1, 3, 1)) +\n  labs(x = \"Heights of People in My Sample\", \n       y = \"Number of People in the Sample\",\n       title = \"Distribution of Heights in My Sample\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe sample distribution is the distribution of heights in my sample.\nThe mean of the sample of heights is 5.482 and the standard deviation is 0.593. This is the sample mean and the sample sd, the sample statistics. The sample mean is not going to be exactly the same as the population parameter and will differ from sample to sample.\n\nmean(sample_1$heights)\nsd(sample_1$heights) \n\n\nConfidence Intervals\n\nFirst sample\nBelow is the the 95% confidence interval of the sample mean of the sample I drew above of 10 people:\n\ncalculate_ci &lt;- function(m, sd, n = 10){\n  \n  se &lt;- sd / sqrt(n)\n  ci &lt;-  m + c(-1, 1) * 1.96 * se\n  \n  tibble(mean = m,\n         ci_l = ci[1],\n         ci_u = ci[2])\n  \n}\n\nm_s1 &lt;- mean(sample_1$heights)\nsd_s1 &lt;- sd(sample_1$heights) \n\ncalculate_ci(m = m_s1, sd = sd_s1) %&gt;%\n  kable(digits = 3) \n\n\n\n\nmean\nci_l\nci_u\n\n\n\n\n5.482\n5.114\n5.85\n\n\n\n\n\nIn this example, the true population mean is in the CI. Note that the CI does not capture the range of the heights in the population (which is 4 to 7).\n\n\nAnother sample\nI take another sample and estimate the mean and CI:\n\nset.seed(05022021)\nsample_2 &lt;- tibble(heights = sample(heights$heights, size = 10))\n\nm_s2 &lt;- mean(sample_2$heights)\nsd_s2 &lt;- sd(sample_2$heights) \n\ncalculate_ci(m = m_s2, sd = sd_s2) %&gt;%\n  kable(digits = 3) \n\n\n\n\nmean\nci_l\nci_u\n\n\n\n\n5.592\n5.192\n5.992\n\n\n\n\n\nThis CI contains 5.5. Notice that the mean is close to 5.6 which is a bit different from the population parameter 5.5, and different from the sample mean of the first sample.\n\n\nExtreme sample\nI take another sample and this time I am not so lucky and draw 10 quite tall people by chance. I calculate the mean and the CI:\n\ntall_folx &lt;- \n  heights %&gt;%\n  filter(heights &gt; 6)\n\nset.seed(05032021)\n\nsample_3 &lt;- tibble(heights = sample(tall_folx$heights, size = 10))\n\nm_s3 &lt;- mean(sample_3$heights)\nsd_s3 &lt;- sd(sample_3$heights) \n\ncalculate_ci(m = m_s3, sd = sd_s3) %&gt;%\n  kable(digits = 3) \n\n\n\n\nmean\nci_l\nci_u\n\n\n\n\n6.216\n6.086\n6.345\n\n\n\n\n\nThis CI definitely doesn’t capture the 5.5 average population height. It doesn’t say anything about the true population height which we know is 5.5.\nThe sample mean here is different from the sample mean of the first two samples and different from the population parameter 5.5.\nIn applied analysis, we won’t know what the true mean is so we won’t know if the CI contains the true mean.\n\n\n\nThought exercise\nNow as would happen in applied analysis, pretend that we don’t know 5.5 is the true mean. How would we know if CI’s from sample 1, sample 2, and sample 3 contain the true parameter? We wouldn’t know."
  },
  {
    "objectID": "posts/CI/index.html#sampling-distribution",
    "href": "posts/CI/index.html#sampling-distribution",
    "title": "What is a Confidence Interval?",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\nNow I have data from a sample and want to use that data to infer something about a population. The sample is typically much smaller in size than the population, 10 people vs 15 million people in this example, so how do I measure how certain we are about the estimate derived from the sample?\nTo measure the uncertainty in my estimated height, I would rely on something called the sampling distribution. Below is a histogram showing the the distribution of 10,000 sample means from 10,000 samples drawn randomly from the population each with sample size of 10.\n\nset.seed(05042021)\n\nsamples_ci &lt;- \n  rerun(10000, {\n    x &lt;- sample(heights$heights, size = 10)\n    m &lt;- mean(x)\n    sd &lt;- sd(x)\n    n &lt;- 10\n    tibble(mean = m,\n           ci_l = m - 1.96 * sd/ sqrt(n),\n           ci_u = m + 1.96 * sd/ sqrt(n))\n  }) %&gt;%\n  bind_rows()\n\nWarning: `rerun()` was deprecated in purrr 1.0.0.\nℹ Please use `map()` instead.\n  # Previously\nrerun(10000, {\nx &lt;- sample(heights$heights, size = 10)\nm &lt;- mean(x)\nsd &lt;- sd(x)\nn &lt;- 10\ntibble(mean = m, ci_l = m - 1.96 * sd / sqrt(n), ci_u = m + 1.96 * sd / sqrt(\nn))\n})\n\n  # Now\nmap(1:10000, ~ {\nx &lt;- sample(heights$heights, size = 10)\nm &lt;- mean(x)\nsd &lt;- sd(x)\nn &lt;- 10\ntibble(mean = m, ci_l = m - 1.96 * sd / sqrt(n), ci_u = m + 1.96 * sd / sqrt(\nn))\n})\n\nsamples_ci %&gt;%\n  ggplot(aes(x = mean)) +\n  geom_histogram(alpha = 0.5, \n                 bins = 50, \n                 fill = \"dark red\") +\n  labs(x = \"Average Heights from Different Samples\", \n       y = \"Number of Samples\",\n       title = \"Distribution of Average Sample Heights\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis distribution is different from the population distribution. It is not the distribution of heights of all people. It is the distribution of means estimated from different samples - a lot of different samples. And note that I am not drawing all possible samples. I am only drawing 10,000 samples.\nThe sampling distribution is the distribution of a statistic (e.g., sample mean) across all possible samples.\nIn applied analysis, we don’t know what the sampling distribution is as we won’t be able to take repeated samples infinite number of times. Statistical models and assumptions help us get at the sampling distribution which then lets us measure uncertainty in our estimates.\n\nStandard error - measure of uncertainty\nThe standard deviation of the sampling distribution is the standard error. The standard error indicates how precise or certain a statistic is. In applied analysis, we typically estimate the standard error using statistical models and assumptions.\n\nWhat is precision?\nPrecision, in statistics, captures the variability of sample means - deviation of sample means from the mean of sample means. Precision is the opposite of standard error ~ small standard error = high precision. Note that we don’t know the true standard error we estimate it too :D\n\n\n\nCI’s of Sample Means in Sampling Distribution\nIn the graph of the sampling distribution above, some of the sample means are way out there. For example, there are some near 6. The table below shows 5 of these sample means and their corresponding CIs. The CIs don’t contain 5.5, the true mean.\n\nsamples_ci %&gt;%\n  filter(mean &gt; 5.9) %&gt;%\n  tail(n = 5) %&gt;%\n  kable(digits = 3) \n\n\n\n\nmean\nci_l\nci_u\n\n\n\n\n5.911\n5.566\n6.257\n\n\n5.948\n5.565\n6.331\n\n\n5.929\n5.625\n6.234\n\n\n5.929\n5.622\n6.236\n\n\n5.922\n5.529\n6.315\n\n\n\n\n\nBelow are the sample means and CI’s at the lower end of the distribution. These CI’s also don’t contain 5.5.\n\nsamples_ci %&gt;%\n  filter(mean &lt; 5) %&gt;%\n  tail(n = 5) %&gt;%\n  kable(digits = 3) \n\n\n\n\nmean\nci_l\nci_u\n\n\n\n\n4.988\n4.711\n5.265\n\n\n4.914\n4.733\n5.095\n\n\n4.979\n4.621\n5.336\n\n\n4.981\n4.702\n5.261\n\n\n4.976\n4.729\n5.224\n\n\n\n\n\nNote here that the means at the extreme ends of the sampling distribution are from simple random samples of the population. So in real applied analysis, we could get one of those samples with extreme heights by chance and our CI won’t capture the true mean.\n\nThought exercise\nNow as would happen in applied analysis, pretend that we don’t know 5.5 is the true mean. How would we know any of these 10,000 CI’s contain the population parameter? We wouldn’t."
  },
  {
    "objectID": "posts/CI/index.html#what-does-ci-tell-me",
    "href": "posts/CI/index.html#what-does-ci-tell-me",
    "title": "What is a Confidence Interval?",
    "section": "What does CI tell me?",
    "text": "What does CI tell me?\nThe width of the confidence intervals tells me how precise the estimate is. When sample size is small, we won’t get very precise estimates. Estimated average heights will be too noisy, too different from sample to sample. When sample size is large, estimated average heights will be more precise. The table below shows mean and CIs calculated from a random sample of 5 people and from another random sample of 1000 people from the population distribution of heights.\n\nset.seed(05052021)\nsmall_s4 &lt;- sample(heights$heights, size = 5)\nm_s4 &lt;- mean(small_s4)\nsd_s4 &lt;- sd(small_s4)\nn_s4 &lt;- length(small_s4)\n\nset.seed(05062021)\nlarge_s5 &lt;- sample(heights$heights, size = 1000)\nm_s5 &lt;- mean(large_s5)\nsd_s5 &lt;- sd(large_s5)\nn_s5 &lt;- length(large_s5)\n\nsmall &lt;- calculate_ci(m = m_s4, sd = sd_s4, n = n_s4) %&gt;%\n  mutate(n = n_s4)\n\nlarge &lt;- calculate_ci(m = m_s5, sd = sd_s5, n = n_s5) %&gt;%\n  mutate(n = n_s5)\n\nbind_rows(small, large) %&gt;%\n  mutate(width = ci_u - ci_l) %&gt;%\n  select(n, everything()) %&gt;%\n  kable(digits = 3)\n\n\n\n\nn\nmean\nci_l\nci_u\nwidth\n\n\n\n\n5\n5.592\n5.266\n5.919\n0.654\n\n\n1000\n5.490\n5.459\n5.521\n0.062\n\n\n\n\n\nIn the example above, for the sample with \\(n = 5\\), the CI is wide compared to the CI from the sample with \\(n = 1000\\). When we take a small sample, sample means from repeated samples will tend to vary more - we are less certain about the estimate - the estimate is less precise - the CI is wide.\nFor the sample with \\(n = 1000\\), you can see that the CI is not as wide as in the case where \\(n = 5\\). When we take a large sample, sample means from repeated samples will tend to vary less - we are more certain about the estimate - the estimate is more precise - the CI is narrow.\nNo matter how narrow or wide the CI is, with repeated sampling, only 95% of the CIs calculated will capture the true mean.\n\nWhat does it all mean?\nPrecision just tells us that if we were to take repeated samples will the sample means be close to each other or really all over the place."
  },
  {
    "objectID": "posts/CI/index.html#what-does-the-ci-not-tell-me",
    "href": "posts/CI/index.html#what-does-the-ci-not-tell-me",
    "title": "What is a Confidence Interval?",
    "section": "What does the CI not tell me?",
    "text": "What does the CI not tell me?\nThe CI is built around an estimate. It doesn’t say anything about the range of heights of all adult population of Texas, for example.\nAlso in applied analysis, we don’t know the true population mean. We only know that if we were to take infinite repeated samples of the same size under the same-ish condition and construct CI’s around the estimated means, 95% of the intervals will capture the true population mean. How do I know if the interval I created is one of the 95% that do capture the true mean or the one of the 5% that don’t capture the true mean. I don’t. Therefore, from the CI itself I don’t know the location of the true mean.\nCI also does not say anything about my psychological level of confidence."
  },
  {
    "objectID": "posts/CI/index.html#how-do-i-know-then-if-my-estimate-is-close-to-the-true-mean",
    "href": "posts/CI/index.html#how-do-i-know-then-if-my-estimate-is-close-to-the-true-mean",
    "title": "What is a Confidence Interval?",
    "section": "How do I know then if my estimate is close to the true mean?",
    "text": "How do I know then if my estimate is close to the true mean?\nWell you don’t, you just assume you do :) (A version of this joke was told to me by Rob Santos).\nWe methodologists study methods under known data generating conditions and we study them under conditions where certain assumptions or conditions hold and when they don’t. We generate tens of thousands of samples ~ approximately creating the sampling distribution. From simulations, we measure accuracy, bias, precision etc. and we can measure those because we generate data based on true parameters that are known to us.\nThen we recommend what method should be used under what conditions. This is why it is important to evaluate whether assumptions hold or not when you are running analyses :) Those assumptions can tell you how close the estimate is to the true mean.\n\nWait what is precision, accuracy, bias?\n\nConsider the image above. The center of the dart board is the population parameter, the population average height of 5.5. Each of the dart is a sample and where it lands is the sample statistic. The goal is for each statistic to be as close as possible to the center.\nPrecision - Precision indicates that statistics from different samples are close to each other even if all the statistics are far away from the center. Panels B and D in the image above show high precision but note that in B the dart locations are far away from center.\nBias - Bias indicates that the average of the locations of the darts is not close to the center - the true parameter. The darts can be far apart from each other but if you take an average of their locations that should be close to the center for an estimator to be unbiased. Panels C and D indicate low bias but note that in C darts are all over the place - any one sample is super far from the center - but if you take their average that average will be close to the center.\nAccuracy - Accuracy accounts for both bias and precision. The darts on average are close to the center and different darts are located close to each other. Panel D indicates high accuracy - this is the the ideal scenario where different samples don’t vary so much from each other and don’t vary on average from the true parameter. All the different darts land right near the center.\nPanel A in the image above indicates low precision, high bias, and low accuracy.\n\nWhere do CI’s fit in here?\nCI’s give information on precision. We can get a narrow CI under scenario B and under scenario D. CI’s do not give information on accuracy. If the estimated mean is really far off from the true mean as is the case with the sample mean from the really tall people (sample 3), the CI is not going to capture the true mean. The CI is going to shift with that estimate."
  },
  {
    "objectID": "posts/mi_ps/index.html",
    "href": "posts/mi_ps/index.html",
    "title": "Propensity Score Analysis with Multiply Imputed Data",
    "section": "",
    "text": "In this post, I walk through steps of running propensity score analysis when there is missingness in the covariate data. Particularly, I look at multiple imputation and ways to condition on propensity scores estimated with imputed data. The code builds on my earlier post where I go over different ways to handle missing data when conducting propensity score analysis. I go through tidyeval way of dealing with multiply imputed data. Please see MatchThem package for functions that work with multiply imputed data and propensity scores (Pishgar & Greifer, 2020).\nHill (2004) and Mitra & Reiter (2016) examined two distinct ways to condition on the propensity scores estimated on multiply imputed data:"
  },
  {
    "objectID": "posts/mi_ps/index.html#multiple-imputation-across-mi-across",
    "href": "posts/mi_ps/index.html#multiple-imputation-across-mi-across",
    "title": "Propensity Score Analysis with Multiply Imputed Data",
    "section": "Multiple Imputation Across (MI Across)",
    "text": "Multiple Imputation Across (MI Across)\nThis approach involves creating m imputed datasets and then estimating propensity scores within each of the datasets and then averaging each unit’s m propensity scores across the m datasets (Hill, 2004). Stratification, matching or IPW can be implemented using these averaged propensity scores (Hill, 2004). Outcome models that include covariates will need to use the weights or strata derived from the averaged propensity scores and the m sets of covariate values. The weighted regression estimates will then need to be pooled."
  },
  {
    "objectID": "posts/mi_ps/index.html#multiple-imputation-within-mi-within",
    "href": "posts/mi_ps/index.html#multiple-imputation-within-mi-within",
    "title": "Propensity Score Analysis with Multiply Imputed Data",
    "section": "Multiple Imputation Within (MI Within)",
    "text": "Multiple Imputation Within (MI Within)\nThis approach involves creating m imputed datasets and then estimating propensity scores within each of the datasets (Hill, 2004). Instead of averaging the propensity scores across the datasets, this method entails conditioning on the propensity scores within the datasets and running the outcome analyses within each dataset (Hill, 2004). The separate regression estimates have to be pooled."
  },
  {
    "objectID": "posts/mi_ps/index.html#imputed-data",
    "href": "posts/mi_ps/index.html#imputed-data",
    "title": "Propensity Score Analysis with Multiply Imputed Data",
    "section": "Imputed data",
    "text": "Imputed data\nI load the saved RData file and then extract the data that contains each of the 10 imputed data stacked.\n\nload(\"temp_data.RData\")\nimp_dat &lt;- complete(temp_data, action = \"long\")"
  },
  {
    "objectID": "posts/mi_ps/index.html#across",
    "href": "posts/mi_ps/index.html#across",
    "title": "Propensity Score Analysis with Multiply Imputed Data",
    "section": "Across",
    "text": "Across\nThe propensity scores are averaged across the imputations when using the Across method. Thus, I create a density plot showing the distribution of the logit of the propensity scores from one of the imputations (all of the imputations will have the same distribution). The distribution of the propensity scores for drop-outs overlaps with that for the stayers satisfying the common support assumption.\n\nimp_dat_ps %&gt;%\n  mutate(drop = if_else(drop_status == 1, \"Drop-outs\", \"Stayers\"),\n         ps_across_logit = log(ps_across/ (1 - ps_across))) %&gt;%\n  filter(.imp == 1) %&gt;%\n  ggplot(aes(x = ps_across_logit, fill = drop)) +\n  geom_density(alpha = .5) + \n  labs(x = \"Logit Propensity Scores\", y = \"Density\", fill = \"\") + \n  ggtitle(\"Common Support: Across Method\") +\n  theme_bw()"
  },
  {
    "objectID": "posts/mi_ps/index.html#within",
    "href": "posts/mi_ps/index.html#within",
    "title": "Propensity Score Analysis with Multiply Imputed Data",
    "section": "Within",
    "text": "Within\nThe propensity scores estimated for each imputation are used when using the Within method. Below, I create density plots showing distributions of the logit of the propensity scores faceted by the imputation. The common support assumption seems to be satisfied across all the imputations.\nIn other datasets, the common support assumption may be violated. In such a case, certain cases might need to be trimmed from the analysis so there is enough overlap of the distributions.\n\nimp_dat_ps %&gt;%\n  mutate(drop = if_else(drop_status == 1, \"Drop-outs\", \"Stayers\")) %&gt;%\n  ggplot(aes(x = ps_logit, fill = drop)) +\n  geom_density(alpha = .5) + \n  labs(x = \"Logit Propensity Scores\", y = \"Density\", fill = \"\") + \n  ggtitle(\"Common Support: Within Method\") + \n  facet_wrap(~ .imp, ncol = 2) + \n  theme_bw()"
  },
  {
    "objectID": "posts/mi_ps/index.html#across-1",
    "href": "posts/mi_ps/index.html#across-1",
    "title": "Propensity Score Analysis with Multiply Imputed Data",
    "section": "Across",
    "text": "Across\n\nacross_pooled &lt;- calc_pooled(dat = across_res, est = estimate, se = se, df = df)\nacross_pooled %&gt;%\n  kable(digits = 3)\n\n\n\n\nest\nse\ndf\nci_lower\nci_upper\n\n\n\n\n-0.335\n0.038\n93.772\n-0.411\n-0.26"
  },
  {
    "objectID": "posts/mi_ps/index.html#within-1",
    "href": "posts/mi_ps/index.html#within-1",
    "title": "Propensity Score Analysis with Multiply Imputed Data",
    "section": "Within",
    "text": "Within\n\nwithin_pooled &lt;- calc_pooled(dat = within_res, est = estimate, se = se, df = df)\nwithin_pooled %&gt;%\n  kable(digits = 3)\n\n\n\n\nest\nse\ndf\nci_lower\nci_upper\n\n\n\n\n-0.356\n0.041\n50.002\n-0.438\n-0.273"
  },
  {
    "objectID": "posts/mi_ps/index.html#interpretation",
    "href": "posts/mi_ps/index.html#interpretation",
    "title": "Propensity Score Analysis with Multiply Imputed Data",
    "section": "Interpretation",
    "text": "Interpretation\n\nAcross\nFor students who dropped out, if they drop out of high school, they are expected to score -0.335, 95% CI[-0.411, -0.26] lower on math score in 2012 compared to if they stayed.\n\n\nWithin\nFor students who dropped out, if they drop out of high school, they are expected to score -0.356, 95% CI[-0.438, -0.273] lower on math score in 2012 compared to if they stayed."
  },
  {
    "objectID": "posts/continuous_ps/index.html",
    "href": "posts/continuous_ps/index.html",
    "title": "Continuous Treatment in Propensity Score Analysis",
    "section": "",
    "text": "In my qualifying exam, in the written part, I was asked about how to analyze the effect of continuous, not binary, treatment using propensity score analysis. I skipped it for the written but I spent a few days looking up how to analyze this in case I would be asked during my oral examination. Sadly, no one asked me even when I asked them to, so here is a blog detailing my explorations."
  },
  {
    "objectID": "posts/continuous_ps/index.html#loading-the-data",
    "href": "posts/continuous_ps/index.html#loading-the-data",
    "title": "Continuous Treatment in Propensity Score Analysis",
    "section": "Loading the Data",
    "text": "Loading the Data\n\nlibrary(tidyverse)\n\ndat &lt;- read_csv(\"https://raw.githubusercontent.com/meghapsimatrix/datasets/master/causal/HSLS09_complete.csv\")"
  },
  {
    "objectID": "posts/continuous_ps/index.html#the-numerators",
    "href": "posts/continuous_ps/index.html#the-numerators",
    "title": "Continuous Treatment in Propensity Score Analysis",
    "section": "The Numerators",
    "text": "The Numerators\nHere I am getting the numerators of the IPW, the marginal densities. I have regressed math_efficacy on just the intercept and used dnorm function to extract the densities.\n\n# the numerator\nmod_num &lt;- lm(math_efficacy ~ 1, data = dat)\n\nnum &lt;- dnorm(x = dat$math_efficacy, # treatment \n             mean = fitted.values(mod_num), # fitted values\n             sd = summary(mod_num)$sigma) # model sigma"
  },
  {
    "objectID": "posts/continuous_ps/index.html#the-denominators",
    "href": "posts/continuous_ps/index.html#the-denominators",
    "title": "Continuous Treatment in Propensity Score Analysis",
    "section": "The Denominators",
    "text": "The Denominators\nHere I am getting the denominators of the IPW, the conditional densities. I have regressed math_efficacy on \\(X\\) and used dnorm function to extract the densities. I am not quite sure whether to use the model sigma which divides the sum of errors squared by the degrees of freedom before taking the square root or whether I should just take the standard deviation of the errors. However, with large sample size the difference between the two are negligible.\n\n# the demonimator\nmod_den &lt;- lm(math_efficacy ~ sex + race + language + repeated_grade + IEP + locale + region + SES, data = dat)\n\nden &lt;- dnorm(x = dat$math_efficacy, # treatment variable\n             mean = fitted.values(mod_den), # fitted values\n             sd = summary(mod_den)$sigma)"
  },
  {
    "objectID": "posts/continuous_ps/index.html#the-ipw",
    "href": "posts/continuous_ps/index.html#the-ipw",
    "title": "Continuous Treatment in Propensity Score Analysis",
    "section": "The IPW",
    "text": "The IPW\nBelow I calculate the stabilized weights:\n\ndat &lt;- dat %&gt;%\n  mutate(ipw_s = num/den)\n\nsummary(dat$ipw_s)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1398  0.9186  0.9778  1.0001  1.0390  5.9782"
  },
  {
    "objectID": "posts/continuous_ps/index.html#checking-balance-and-outcome-analysis",
    "href": "posts/continuous_ps/index.html#checking-balance-and-outcome-analysis",
    "title": "Continuous Treatment in Propensity Score Analysis",
    "section": "Checking Balance and Outcome Analysis",
    "text": "Checking Balance and Outcome Analysis\nShort story: For balance, we have to calculate weighted correlations, and for outcome analysis we estimate the expected outcome for each treatment level and compare (Austin, 2019)."
  },
  {
    "objectID": "publications/cwb/index.html",
    "href": "publications/cwb/index.html",
    "title": "Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies",
    "section": "",
    "text": "The most common and well-known meta-regression models work under the assumption that there is only one effect size estimate per study and that the estimates are independent. However, meta-analytic reviews of social science research often include multiple effect size estimates per primary study, leading to dependence in the estimates. Some meta-analyses also include multiple studies conducted by the same lab or investigator, creating another potential source of dependence. An increasingly popular method to handle dependence is robust variance estimation (RVE), but this method can result in inflated Type I error rates when the number of studies is small. Small-sample correction methods for RVE have been shown to control Type I error rates adequately but may be overly conservative, especially for tests of multiple-contrast hypotheses. We evaluated an alternative method for handling dependence, cluster wild bootstrapping, which has been examined in the econometrics literature but not in the context of meta-analysis. Results from two simulation studies indicate that cluster wild bootstrapping maintains adequate Type I error rates and provides more power than extant small sample correction methods, particularly for multiple-contrast hypothesis tests. We recommend using cluster wild bootstrapping to conduct hypothesis tests for meta-analyses with a small number of studies. We have also created an R package that implements such tests.\n\n\n\n Back to topCitationBibTeX citation:@article{joshi2022,\n  author = {Joshi, Megha and Pustejovsky, James E. and Beretvas, S.\n    Natasha},\n  title = {Cluster Wild Bootstrapping to Handle Dependent Effect Sizes\n    in Meta-Analysis with a Small Number of Studies},\n  journal = {Research Synthesis Methods},\n  volume = {13},\n  number = {4},\n  pages = {457-477},\n  date = {2022-02-25},\n  url = {https://doi.org/10.1002/jrsm.1554},\n  doi = {10.1002/jrsm.1554},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJoshi, M., Pustejovsky, J. E., & Beretvas, S. N. (2022). Cluster\nwild bootstrapping to handle dependent effect sizes in meta-analysis\nwith a small number of studies. Research Synthesis Methods,\n13(4), 457–477. https://doi.org/10.1002/jrsm.1554"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Megha Joshi",
    "section": "",
    "text": "Causal Inference\nMeta-analysis\nBootstrapping\nMissing data\nMachine learning\n\n\n\n\n PhD in Quantitative Methods, 2021  The University of Austin at Texas\n BA in Art History and Psychology, 2014  Bryn Mawr College"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Megha Joshi",
    "section": "",
    "text": "Causal Inference\nMeta-analysis\nBootstrapping\nMissing data\nMachine learning"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Megha Joshi",
    "section": "",
    "text": "PhD in Quantitative Methods, 2021  The University of Austin at Texas\n BA in Art History and Psychology, 2014  Bryn Mawr College"
  },
  {
    "objectID": "software/wildmeta/index.html",
    "href": "software/wildmeta/index.html",
    "title": "wildmeta",
    "section": "",
    "text": "Typical methods to conduct meta-analysis—pooling effect sizes or analyzing moderating effects with meta-regression—work under the assumption that the effect size estimates are independent. However, primary studies often report multiple estimates of effect sizes. Presence of multiple effect sizes leads to dependence as the estimates within each study are likely correlated (e.g., because the same participants provide multiple outcome scores). The increasingly popular method to handle such dependence, robust variance estimation (RVE), results in inflated Type 1 error rate when the number of studies is small (Hedges, Tipton & Johnson, 2010; Tipton, 2015).\nTipton (2015) and Tipton & Pustejovsky (2015) examined several small sample correction methods. Tipton (2015) recommended CR2 type correction for RVE as well as the use of Satterthwaite degrees of freedom for single coefficient tests. Tipton & Pustejovsky (2015) examined corrections for multiple-contrast hypothesis tests. The authors found that the HTZ test, which is an extension of the CR2 correction method with the Satterthwaite degrees of freedom, controlled Type 1 error rate adequately even when the number of studies was small. However, Joshi, Pustejovsky & Beretvas (2021) showed, through simulations, that the HTZ test can be conservative. We examined another method, cluster wild bootstrapping (CWB), that has been studied in the econometrics literature but not in the meta-analytic context. The results of the simulations from Joshi, Pustejovsky & Beretvas (2021) showed that CWB adequately controlled for Type 1 error rate and had more power than the HTZ test especially for multiple-contrast hypothesis tests.The goal of this package is to provide applied meta-analytic researchers a function with which they can conduct single coefficient tests or multiple-contrast hypothesis tests using cluster wild bootstrapping.\n\n\n\n Back to topCitationBibTeX citation:@software{joshi2023,\n  author = {Joshi, Megha and Pustejovsky, James E. and Cappelli, Pierce},\n  title = {Wildmeta},\n  version = {0.3.2},\n  date = {2023-03-08},\n  url = {https://fancy-zuccutto-29a372.netlify.app/software/wildmeta/},\n  doi = {10.32614/CRAN.package.wildmeta},\n  note = {R package},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJoshi, M., Pustejovsky, J. E., & Cappelli, P. (2023).\nwildmeta [R package]. https://doi.org/10.32614/CRAN.package.wildmeta"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "wildmeta\n\n\n\n\n\n\n\n\n\n\n\nJul 26, 2024\n\n\nMegha Joshi, James E. Pustejovsky, Pierce Cappelli\n\n\n\n\n\n\n\n\n\n\n\n\nsimhelpers\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\nMegha Joshi, James E. Pustejovsky\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]